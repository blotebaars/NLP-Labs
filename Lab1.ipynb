{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31652342-ff63-4858-b8b6-66c60f854c8a",
   "metadata": {
    "id": "31652342-ff63-4858-b8b6-66c60f854c8a"
   },
   "source": [
    "## TI3160TU: Natural Language Processing - Basic Text Preprocessing Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9194f-cd52-43a3-8e76-bdbfc76d20e1",
   "metadata": {
    "id": "75f9194f-cd52-43a3-8e76-bdbfc76d20e1"
   },
   "source": [
    "In this hands-on lab we will dive into the main preprocessing steps when preparing data for use in various NLP techniques. Particularly, we are going to see how to implement and run:\n",
    "1. **Regular Expressions (Regex)**\n",
    "2. **Word Tokenization & Sentence Segmentation**\n",
    "3. **Lemmatization and Stemming**\n",
    "4. **Basic text similarity metrics based on Hamming Distance, Levenshtein Distance, and Jaccard Index**\n",
    "\n",
    "\n",
    "For the purposes of this lab, we will load some data from Reddit and use it to implement and run the abovementioned steps. Particularly, the dataset includes all the comments made by Reddit users on the /r/TUDelft subreddit between June 2022 and December 2022.\n",
    "**Make sure that the data file (comments_TUDelft.ndjson) is in the same directory as the Jupyter Notebook. In case you are using online platforms such as Collab, make sure to first upload the data file before running the cells below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284fe9b-41f9-41a7-a0bd-e6f0dfeb5c0f",
   "metadata": {
    "id": "1284fe9b-41f9-41a7-a0bd-e6f0dfeb5c0f"
   },
   "source": [
    "### 0. Preparation and Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2en9lDXba5w6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2en9lDXba5w6",
    "outputId": "7feac4d4-b9ed-4a52-b1d3-e1adc3d9c5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tldextract in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (3.4.4)\n",
      "Requirement already satisfied: idna in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from tldextract) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from tldextract) (2.28.1)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from tldextract) (3.12.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from requests>=2.1.0->tldextract) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from requests>=2.1.0->tldextract) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/szannettou/Library/Python/3.9/lib/python/site-packages (from requests>=2.1.0->tldextract) (2022.9.24)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from requests-file>=1.4->tldextract) (1.15.0)\n",
      "\u001b[33mDEPRECATION: jupyter-server 2.0.0 has a non-standard dependency specifier jupyter-core!=~5.0,>=4.12. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of jupyter-server or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/szannettou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that we have installed the required packages for this lab\n",
    "!pip install tldextract # package that is used to extract domains from URLs\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cf1f31-b464-4bbe-afac-63f37a74679f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6cf1f31-b464-4bbe-afac-63f37a74679f",
    "outputId": "e5d9ef96-c419-4f0d-bd9a-a572815565b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Reddit comments! Our dataset includes 2263 Reddit comments!\n"
     ]
    }
   ],
   "source": [
    "# we need the library json as the reddit data is stored in line-delimited json objects\n",
    "# (one json object in each line, with each line representing a Reddit comment)\n",
    "import json\n",
    "\n",
    "# function to load all comment data into a list of strings\n",
    "# Input: the path of the file including our data\n",
    "# Output: a list of strings including the body of the Reddit comments\n",
    "def load_reddit_comment_data(data_directory):\n",
    "\n",
    "    comments_data = [] # list object that will store the loaded Reddit comments\n",
    "\n",
    "    # we first open the file that includes our dataset\n",
    "    with open(data_directory, 'r', encoding='utf-8') as f:\n",
    "        # iterate the file, reading it line by line\n",
    "        for line in f:\n",
    "            # load the data petraining to a line into a json object in memory\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # append the comment\n",
    "            comments_data.append(data['body'])\n",
    "\n",
    "    # the method returns all the loaded Reddit comments\n",
    "    return comments_data\n",
    "\n",
    "# our data is stored in this file\n",
    "data_dir = './comments_TUDelft.ndjson'\n",
    "# lets load our dataset into memory\n",
    "reddit_data = load_reddit_comment_data(data_dir)\n",
    "print(\"Successfully loaded Reddit comments! Our dataset includes %d Reddit comments!\" %len(reddit_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ab2b2-1c93-4ebf-9f43-397a4cb7db66",
   "metadata": {
    "id": "b84ab2b2-1c93-4ebf-9f43-397a4cb7db66",
    "tags": []
   },
   "source": [
    "## 1. Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952da49-d73a-4c86-a394-9c5eaf9956b2",
   "metadata": {
    "id": "8952da49-d73a-4c86-a394-9c5eaf9956b2"
   },
   "source": [
    "Regular expressions are a powerful tool allowing to perform text operations based on a formal language (syntax).\n",
    "Regular expressions can be used to define a variety of rules that allows us to solve various tasks including:\n",
    "* Searching for patterns in text and extracting data from documents\n",
    "* Manipulating text (e.g., substituting text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1b903-2087-46c6-aef4-5a3c1bdcc1f8",
   "metadata": {
    "id": "5fa1b903-2087-46c6-aef4-5a3c1bdcc1f8"
   },
   "source": [
    "## 1.1 Searching for patterns in text and extracting data using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53476563-2b53-456a-bfa1-60a6974c15d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53476563-2b53-456a-bfa1-60a6974c15d0",
    "outputId": "4fed88d5-8719-4bed-e8a8-442d49131442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I got a 40 in IB (international baccalaureate), from maths A HL I got a 5 (1 mark off of a 6) and from physics HL a 6\\n\\nIn the Delft ranking I was 445\\n\\nKeep in mind the selection procedure tests were right around the time I had mocks internal exams at school so I was super busy with that and barely had time to study for the selection procedure.',\n",
       " 'Same, I am an IBer as well. But is it really that you only need 24 for IB to get into Delft if pass the selection procedures?',\n",
       " \"There should be another button allowing you to recharge your account in the main menu. Screenshot: https://imgur.com/a/HyRrCxM \\nMaybe trying to log in from another device will work. If it doesn't you can also go to the printing shop in front of delft centraal\",\n",
       " 'You should contact the TU Delft student ambassadors. Theyâ€™ll connect you with someone',\n",
       " 'Aerospace is a highly competitive BSc. You need to meet the minimum requirements for application. Even when you meet the requirements, you will still have to go through a selection procedure.\\n\\n\\n\\nhttps://www.tudelft.nl/en/education/admission-and-application/bsc-international-diploma/admission-requirements/diploma-with-additional-requirements#c106230\\n\\nFor AE you need Mathematics: A2 (Mechanics) + Physics: A2.\\n\\nIf you want a definitive answer you can contact the Education and Student Affairs (ESA) office at: contactcentre-ESA@tudelft.nl or +31(0)152788012',\n",
       " \"It's all on the TUDelft website. To enter you just have to have solid maths knowledge (verify you learn it all at school and practice the harder parts on your own) and learn boolean algebra. The info for that is on their website... They have a really good book that walks you through the basics.\",\n",
       " 'you can find this information [here](https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/bsc-nederlands-diploma/1-toelatingseisen#c39206). TLDR: all you need for the CS bachelor is the maths B certificate.',\n",
       " \"If you can get the housing service you absolutely should take it! There is a huge housing problem in and around delft so there is a very real chanse that you won't be able to find a place or have to get a way more expensive place. So unless you can stay at families place if housing falls through defenitly make use of it.\\n\\nI do not know whether you can have access to the housing earlier or later, I would recommend sending an email to the tu for that information.\\n\\nAlso since you say that every penny counts, be aware that the cost of living is quite high in the Netherlands. The uni's estimate is anywhere from 800-1100 euros a month for a student, so do keep this in mind.\",\n",
       " 'Thank you for the advise. Yes, it is the \"TU Delftâ€™s housing portal\" which as I understand it works through a partnership with DUWO',\n",
       " \"At Delft grades are rounded to the nearest half point. So the lowest passing grade, which rounds up to a 6, would be a 5.8 since a 5.75, the true lowest passing grade, isn't given.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For working with regular expresssions we are going to use the \"re\" library.\n",
    "# The library allow us to define and run regular expressions in our datasets.\n",
    "import re\n",
    "\n",
    "# function that performs a search pattern in our reddit dataset\n",
    "# INPUTS: the regular expression pattern and the input dataset\n",
    "# OUTPUT: a list with all Reddit comments matching the regular expression\n",
    "def search_pattern(pattern, data):\n",
    "\n",
    "    # list to store our matching comments\n",
    "    matches = []\n",
    "\n",
    "    # iterate through the comments\n",
    "    for comment in data:\n",
    "\n",
    "        # search for our regex pattern\n",
    "        if re.search(regex_pattern, comment):\n",
    "\n",
    "            # lets store the comment in our list\n",
    "            matches.append(comment)\n",
    "\n",
    "    # return the matching comments\n",
    "    return matches\n",
    "\n",
    "# lets try to do a simple search by finding all comments mentioning \"delft\"\n",
    "regex_pattern = '[dD]elft' # we define our regex pattern and we want to make sure that we either match delft or Delft\n",
    "\n",
    "# we run our function to extract all comments mentioning Delft/delft\n",
    "comments_mentioning_delft = search_pattern(regex_pattern, reddit_data)\n",
    "comments_mentioning_delft[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82567dae-374c-4f4e-b53f-5d2fcf29a8b5",
   "metadata": {
    "id": "82567dae-374c-4f4e-b53f-5d2fcf29a8b5"
   },
   "source": [
    "The result of the **re.search** operation is more than a Boolean operator. It provides more information about the match\n",
    "For instance, it can give us information on where the match happened in the string\n",
    "Lets see an example with the comments including Delft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6c8b23-3c76-46ff-9c32-4b187bfce3c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea6c8b23-3c76-46ff-9c32-4b187bfce3c4",
    "outputId": "f28ac1ea-71e1-4d11-eb1e-10dc876b0d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start of the match is on character 126, while the end of the match is in the character 131\n"
     ]
    }
   ],
   "source": [
    "# repeat the same search in the first document that we already knows that includes the term delft/Delft\n",
    "match = re.search(regex_pattern, comments_mentioning_delft[0])\n",
    "\n",
    "# print the start and end of the match in the first document\n",
    "print(\"The start of the match is on character %d, while the end of the match is in the character %d\" %(match.start(), match.end()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e2bc3-4dd7-42fa-982d-1a9bcd2c1f88",
   "metadata": {
    "id": "883e2bc3-4dd7-42fa-982d-1a9bcd2c1f88"
   },
   "source": [
    "Note that the **re.search** method provides information only for the first match.\n",
    "So the questions that arises is how we deal with a document that has multiple occurrences of the pattern we are searching for?\n",
    "In cases where we want to find information about all the occurrences, we need to use the **re.finditer** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d807b820-2dd7-4595-ac04-c0d43ff6ab43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "d807b820-2dd7-4595-ac04-c0d43ff6ab43",
    "outputId": "8a1d1b0f-8e20-4dc8-c623-c22c65a62365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall we find 437 matches for Delft/delft in our dataset! \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_index</th>\n",
       "      <th>comment</th>\n",
       "      <th>match_start</th>\n",
       "      <th>match_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>I got a 40 in IB (international baccalaureate), from maths A HL I got a 5 (1 mark off of a 6) an...</td>\n",
       "      <td>126</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Same, I am an IBer as well. But is it really that you only need 24 for IB to get into Delft if p...</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>There should be another button allowing you to recharge your account in the main menu. Screensho...</td>\n",
       "      <td>245</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>You should contact the TU Delft student ambassadors. Theyâ€™ll connect you with someone</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>Aerospace is a highly competitive BSc. You need to meet the minimum requirements for application...</td>\n",
       "      <td>209</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>Aerospace is a highly competitive BSc. You need to meet the minimum requirements for application...</td>\n",
       "      <td>531</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>It's all on the TUDelft website. To enter you just have to have solid maths knowledge (verify yo...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>you can find this information [here](https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/bs...</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41</td>\n",
       "      <td>If you can get the housing service you absolutely should take it! There is a huge housing proble...</td>\n",
       "      <td>112</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46</td>\n",
       "      <td>Thank you for the advise. Yes, it is the \"TU Delftâ€™s housing portal\" which as I understand it wo...</td>\n",
       "      <td>45</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77</td>\n",
       "      <td>At Delft grades are rounded to the nearest half point. So the lowest passing grade, which rounds...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>86</td>\n",
       "      <td>You should check their website for  career and counseling services. Any recruiter or advisor in ...</td>\n",
       "      <td>96</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>87</td>\n",
       "      <td>[https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-doc...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>87</td>\n",
       "      <td>[https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-doc...</td>\n",
       "      <td>126</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116</td>\n",
       "      <td>TU Delft will not consider your grades. They will rank you based on your scores on the entry exa...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>129</td>\n",
       "      <td>You open Google, find the TU Delft Aerospace website and start reading.\\n\\nhttps://www.tudelft.n...</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>129</td>\n",
       "      <td>You open Google, find the TU Delft Aerospace website and start reading.\\n\\nhttps://www.tudelft.n...</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>130</td>\n",
       "      <td>Try contactcentre-esa@tudelft.nl. They can probably tell you if the system is down or if there i...</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>132</td>\n",
       "      <td>Then please be more specific in your question.\\n\\nI cannot find the application dates you've men...</td>\n",
       "      <td>249</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>137</td>\n",
       "      <td>As long as you are an Italian citizen (and thus a citizen of a EU member state), the tuition fee...</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comment_index  \\\n",
       "0               6   \n",
       "1               7   \n",
       "2              19   \n",
       "3              28   \n",
       "4              29   \n",
       "5              29   \n",
       "6              32   \n",
       "7              35   \n",
       "8              41   \n",
       "9              46   \n",
       "10             77   \n",
       "11             86   \n",
       "12             87   \n",
       "13             87   \n",
       "14            116   \n",
       "15            129   \n",
       "16            129   \n",
       "17            130   \n",
       "18            132   \n",
       "19            137   \n",
       "\n",
       "                                                                                                comment  \\\n",
       "0   I got a 40 in IB (international baccalaureate), from maths A HL I got a 5 (1 mark off of a 6) an...   \n",
       "1   Same, I am an IBer as well. But is it really that you only need 24 for IB to get into Delft if p...   \n",
       "2   There should be another button allowing you to recharge your account in the main menu. Screensho...   \n",
       "3                 You should contact the TU Delft student ambassadors. Theyâ€™ll connect you with someone   \n",
       "4   Aerospace is a highly competitive BSc. You need to meet the minimum requirements for application...   \n",
       "5   Aerospace is a highly competitive BSc. You need to meet the minimum requirements for application...   \n",
       "6   It's all on the TUDelft website. To enter you just have to have solid maths knowledge (verify yo...   \n",
       "7   you can find this information [here](https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/bs...   \n",
       "8   If you can get the housing service you absolutely should take it! There is a huge housing proble...   \n",
       "9   Thank you for the advise. Yes, it is the \"TU Delftâ€™s housing portal\" which as I understand it wo...   \n",
       "10  At Delft grades are rounded to the nearest half point. So the lowest passing grade, which rounds...   \n",
       "11  You should check their website for  career and counseling services. Any recruiter or advisor in ...   \n",
       "12  [https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-doc...   \n",
       "13  [https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-doc...   \n",
       "14  TU Delft will not consider your grades. They will rank you based on your scores on the entry exa...   \n",
       "15  You open Google, find the TU Delft Aerospace website and start reading.\\n\\nhttps://www.tudelft.n...   \n",
       "16  You open Google, find the TU Delft Aerospace website and start reading.\\n\\nhttps://www.tudelft.n...   \n",
       "17  Try contactcentre-esa@tudelft.nl. They can probably tell you if the system is down or if there i...   \n",
       "18  Then please be more specific in your question.\\n\\nI cannot find the application dates you've men...   \n",
       "19  As long as you are an Italian citizen (and thus a citizen of a EU member state), the tuition fee...   \n",
       "\n",
       "    match_start  match_end  \n",
       "0           126        131  \n",
       "1            86         91  \n",
       "2           245        250  \n",
       "3            26         31  \n",
       "4           209        214  \n",
       "5           531        536  \n",
       "6            18         23  \n",
       "7            51         56  \n",
       "8           112        117  \n",
       "9            45         50  \n",
       "10            3          8  \n",
       "11           96        101  \n",
       "12           15         20  \n",
       "13          126        131  \n",
       "14            3          8  \n",
       "15           29         34  \n",
       "16           87         92  \n",
       "17           24         29  \n",
       "18          249        254  \n",
       "19          104        109  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the pandas library that helps us store and visualize data in a tabular format\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "def search_all_pattern(pattern, data):\n",
    "\n",
    "    # list to store our data\n",
    "    all_matches = []\n",
    "\n",
    "    # keep track of the comment index to store it in the output (acts as a unique identifier for each comment in this simple example)\n",
    "    i = 0\n",
    "\n",
    "    # iterate through the comments\n",
    "    for comment in data:\n",
    "\n",
    "        # search for our regex pattern\n",
    "        matches = re.finditer(pattern, comment)\n",
    "\n",
    "        # iterate through all matches in the comment\n",
    "        for match in matches:\n",
    "            # lets store all and the matches start and end in a json object in our list (one json object for each match)\n",
    "            all_matches.append({'comment_index': i, 'comment': comment, \"match_start\": match.start(), \"match_end\": match.end()})\n",
    "\n",
    "        # increase the comment index\n",
    "        i+=1\n",
    "    # return the matching comments and the start/end of each match in a pandas DataFrame\n",
    "    return pd.DataFrame(all_matches)\n",
    "\n",
    "# run our function to get all the matches in our Reddit dataset\n",
    "all_delft_matches = search_all_pattern(regex_pattern, reddit_data)\n",
    "\n",
    "# print how many matches we found\n",
    "print(\"Overall we find %d matches for Delft/delft in our dataset! \" %(all_delft_matches.shape[0]))\n",
    "\n",
    "# lets also print some examples (the first two rows in our pandas DataFrame)\n",
    "all_delft_matches[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcb475-5570-453c-b3fe-61d1a0ad6325",
   "metadata": {
    "id": "eafcb475-5570-453c-b3fe-61d1a0ad6325"
   },
   "source": [
    "Observe for example that in this output we have comments with multiple appearances of our matching pattern. For instance, lets look at the comment with index 129 (it has two entries in our DataFrame, indicating that our pattern appeared twice in the comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d169b5e6-046e-41d1-839c-fb2e67a385eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "d169b5e6-046e-41d1-839c-fb2e67a385eb",
    "outputId": "5bddd270-d260-42be-b405-b254396d5673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You open Google, find the TU Delft Aerospace website and start reading.\\n\\nhttps://www.tudelft.nl/en/onderwijs/opleidingen/bachelors/aerospace-engineering/bsc-aerospace-engineering\\n\\nThe website is quite extensive and enrollment procedure is detailed there. Please put some effort into your question next time.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data[129]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ac6fb-3dde-4d24-aa2f-e72038770107",
   "metadata": {
    "id": "397ac6fb-3dde-4d24-aa2f-e72038770107"
   },
   "source": [
    "### Use case: Extract all URLs that are mentioned in our Reddit dataset and find the most popular domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44077f2-c76d-40f4-9416-808a466af2eb",
   "metadata": {
    "id": "b44077f2-c76d-40f4-9416-808a466af2eb"
   },
   "source": [
    "Lets assume that we want to study what links (i.e., URLs) are shared by Reddit users on the /r/TUDelft subreddit. Lets see how we can use the power of regular expressions to achieve this goal. We are going to use regular expressions to extract all URLs and then use an off-the-shelf tool to convert URLs to domains and find the most popular domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c882eae3-667c-46f2-9c2d-5bd69304f13b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c882eae3-667c-46f2-9c2d-5bd69304f13b",
    "outputId": "dc484f44-8fa3-4d06-c322-7ce3ee9cf534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall we find 155 URLs in our dataset!\n",
      "Some examples below...\n",
      "['https://imgur.com/a/HyRrCxM', 'https://www.tudelft.nl/en/education/admission-and-application/bsc-international-diploma/admission-requirements/diploma-with-additional-requirements#c106230', 'https://www.stud.nl/)', 'https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/bsc-nederlands-diploma/1-toelatingseisen#c39206).', 'https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-documents#c425773](https://www.tudelft.nl/onderwijs/toelating-en-aanmelding/msc-international-diploma/required-documents#c425773)', 'https://www.tudelft.nl/en/onderwijs/opleidingen/bachelors/aerospace-engineering/bsc-aerospace-engineering', 'https://www.tudelft.nl/en/education/admission-and-application/bsc-international-diploma/application-procedure', 'https://oras.nl/post/opening-hours-2/', 'https://www.reddit.com/r/TUDelft/comments/sv34r7/are_tuition_fees_paid_yearly_or_every_semester/hxfd7jc/):', 'https://www.nibud.nl/consumenten/wat-kost-studeren/)']\n"
     ]
    }
   ],
   "source": [
    "# function that extracts all URLs that appear in a list of strings\n",
    "# Input: a dataset in the form of a list of strings\n",
    "# Output: a list of URLs that appear in the dataset\n",
    "def extract_urls(data):\n",
    "    # simple regular expression for URLs. does not cover all possible URL formats :)\n",
    "    regex = '(https?://\\S+)'\n",
    "\n",
    "    # list to store the results\n",
    "    urls = []\n",
    "\n",
    "    # iterate through the dataset\n",
    "    for comment in data:\n",
    "\n",
    "        # the findall method returns all matches in a list of strings\n",
    "        matches = re.findall(regex, comment)\n",
    "\n",
    "        # store all the matches in our list\n",
    "        urls.extend(matches)\n",
    "\n",
    "    # return the result\n",
    "    return urls\n",
    "\n",
    "# use our function to extract the URLs in our data\n",
    "all_urls = extract_urls(reddit_data)\n",
    "\n",
    "# print some stats/results\n",
    "print(\"Overall we find %d URLs in our dataset!\" %(len(all_urls)))\n",
    "print(\"Some examples below...\")\n",
    "print(all_urls[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85db0805-83c9-4912-81d9-c067694f62a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85db0805-83c9-4912-81d9-c067694f62a0",
    "outputId": "660519d7-8208-4900-fc1a-ebd5a94b52fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tudelft.nl', 74),\n",
       " ('reddit.com', 14),\n",
       " ('cloudfront.net', 6),\n",
       " ('whatsapp.com', 5),\n",
       " ('belastingdienst.nl', 3),\n",
       " ('wikipedia.org', 3),\n",
       " ('doorstroommatrix.nl', 3),\n",
       " ('github.com', 2),\n",
       " ('oweedelft.com', 2),\n",
       " ('ns.nl', 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tldextract # library that will help us convert URLs to domains\n",
    "from collections import Counter # library that provides some easy-to-use tools to perform various count operations\n",
    "\n",
    "# function that extracts the domain given a URL\n",
    "# Input: a string corresponding to a URL\n",
    "# Output: a string corresponding to the domain of the given URL\n",
    "def extract_domain(url):\n",
    "    return tldextract.extract(url).registered_domain\n",
    "\n",
    "# use list comprehension to run our extract_domain function for each URL in our dataset\n",
    "all_domains = [extract_domain(x) for x in all_urls]\n",
    "\n",
    "# use the Counter library to extract the 10 most common domains in our dataset along with the number of appearances (list of tuples)\n",
    "Counter(all_domains).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da89edf-2e6b-42c3-a873-cd49a24424c3",
   "metadata": {
    "id": "6da89edf-2e6b-42c3-a873-cd49a24424c3"
   },
   "source": [
    "From this basic analysis using regular expressions we can observe that the most popular domains by users commenting on the /r/TUDelft subreddit are unsuprisingly, the TUDelft website (74 appearances), followed by URLs that point to other resources in the Reddit platform (14 appearances)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36225e-bced-4914-a953-e8482e7240ec",
   "metadata": {
    "id": "0a36225e-bced-4914-a953-e8482e7240ec"
   },
   "source": [
    "### **Exercise:** Given this particular Reddit dataset, use regular expressions to find all posts talking about Architecture, Aerospace, or TPM/TBM. Comment on which is the most popular faculty/topic among these three and inspect the matching comments to see how well your regular expressions work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2923bcc-884c-484d-bc43-676869c38869",
   "metadata": {
    "id": "e2923bcc-884c-484d-bc43-676869c38869"
   },
   "source": [
    "**Tip:** You can re-use the *search_all_pattern* method that we implemented above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e74a07-44aa-43bd-a4e5-4546d32d65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32e1a8-b496-4cb7-b847-9372e56a3103",
   "metadata": {
    "id": "9e32e1a8-b496-4cb7-b847-9372e56a3103"
   },
   "source": [
    "## 1.2 Manipulating text using regular expressions (we focus on substitution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca5499-8bf3-47f2-957a-58f80c90f1fe",
   "metadata": {
    "id": "9dca5499-8bf3-47f2-957a-58f80c90f1fe"
   },
   "source": [
    "In many cases, we want to normalize the text before processing it. To achieve this, we can use regular expressions to manipulate text and make it more uniform. Some examples include:\n",
    "1. Converting British English to US English (or vice-versa depending on use-case)\n",
    "2. Changing the case of some characters (e.g., delft -> Delft)\n",
    "\n",
    "Lets see an example on how we can perform text substitution using regular expressions in Python using the simplified example of delft -> Delft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db9722b-d3ab-497a-aad4-e6a2b43bb06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8db9722b-d3ab-497a-aad4-e6a2b43bb06c",
    "outputId": "2cd5c1c0-a330-4e8f-f08f-7a3a5795f24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of occurrences for delft=170 and Delft=267\n"
     ]
    }
   ],
   "source": [
    "# first lets see how many times each word appears in our dataset\n",
    "num_matches_delft = search_all_pattern('delft', reddit_data).shape[0]\n",
    "num_matches_Delft = search_all_pattern('Delft', reddit_data).shape[0]\n",
    "print(\"Numbers of occurrences for delft=%d and Delft=%d\" %(num_matches_delft, num_matches_Delft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e03192e-0d45-48a2-bf49-b51d400f7479",
   "metadata": {
    "id": "8e03192e-0d45-48a2-bf49-b51d400f7479"
   },
   "outputs": [],
   "source": [
    "# a function that substitutes the regex matches with the new word\n",
    "# Input: a regular expression to match and the new word that corresponds to the replaced regex and the data to run the substitutions on\n",
    "# Output: a list of the data with the replacements as described by the regex and the new word\n",
    "def substitute_using_regex(regex, new_word, data):\n",
    "\n",
    "    # list to hold the result (i.e., the list of substituted comments)\n",
    "    results = []\n",
    "\n",
    "    # iterate through the comments in our dataset\n",
    "    for comment in data:\n",
    "\n",
    "        # we use the re.sub method to replace text matching the regex with the new_word for the current comment\n",
    "        substituted_text = re.sub(regex, new_word, comment)\n",
    "\n",
    "        # store the substituted text into our results list\n",
    "        results.append(substituted_text)\n",
    "\n",
    "    # return the results\n",
    "    return results\n",
    "\n",
    "delft_pattern = 'delft' # we want to match everything that has delft so that we convert it to Delft\n",
    "new_delft_word = 'Delft' # this is the string that we want to have in the normalized dataset\n",
    "\n",
    "# call our function to replace all instances of delft to Delft\n",
    "reddit_data_substituted = substitute_using_regex(delft_pattern, new_delft_word, reddit_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6a779-e347-4fe1-a3a9-ee1d45aeb36b",
   "metadata": {
    "id": "32e6a779-e347-4fe1-a3a9-ee1d45aeb36b"
   },
   "source": [
    "Now, Lets verify our results using our previously defined *search_all_pattern* (returns all matches given a specific regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2511c97d-366b-434c-a2ae-783bec2eaddb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2511c97d-366b-434c-a2ae-783bec2eaddb",
    "outputId": "6c9e069f-5c90-49b4-c87e-a1c5b654ad9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of occurrences for delft=0 and Delft=437\n"
     ]
    }
   ],
   "source": [
    "num_matches_delft = search_all_pattern('delft', reddit_data_substituted).shape[0]\n",
    "num_matches_Delft = search_all_pattern('Delft', reddit_data_substituted).shape[0]\n",
    "print(\"Numbers of occurrences for delft=%d and Delft=%d\" %(num_matches_delft, num_matches_Delft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792080-c41a-4c8f-8d66-30d3651ce6fa",
   "metadata": {
    "id": "48792080-c41a-4c8f-8d66-30d3651ce6fa"
   },
   "source": [
    "**Use case:** In many applications, when doing analysis on large corpora of text we want to preprocess the data and remove some parts. For instance, we might want to remove URLs from our data. Here, we will see how we can use a similar approach to the above to remove all URLs from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad82645-e9ae-4dcd-b73e-a4570e6511f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ad82645-e9ae-4dcd-b73e-a4570e6511f2",
    "outputId": "ff7f635d-8d1a-4d89-df38-30aabd9d5fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URL matches in the raw dataset = 155\n"
     ]
    }
   ],
   "source": [
    "regex_url = '(https?://\\S+)' # simple regular expression for URLs\n",
    "\n",
    "# lets check how many URL apperances we have in our raw dataset (before preprocessing)\n",
    "print(\"Number of URL matches in the raw dataset = %d\" %(search_all_pattern(regex_url, reddit_data).shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79c4382e-0bb9-462e-8047-8fe115364e56",
   "metadata": {
    "id": "79c4382e-0bb9-462e-8047-8fe115364e56"
   },
   "outputs": [],
   "source": [
    "\n",
    "# here, we are re-using the same function as before. we provide our simple regex to match the URLs and for new word we simply give the empty string\n",
    "# this corresponds to deleting the URLs, as the URLs will be substituted with an empty string\n",
    "reddit_data_no_url = substitute_using_regex(regex_url, '', reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47b21b50-ede7-41e6-a447-f5c8df44bec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47b21b50-ede7-41e6-a447-f5c8df44bec7",
    "outputId": "39715f0f-73cd-4a73-b624-a6de2527fd08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URL matches in the preprocessed dataset = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of URL matches in the preprocessed dataset = %d\" %(search_all_pattern(regex_url, reddit_data_no_url).shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b0104-ce75-4b9f-81c8-53dad0e63a44",
   "metadata": {
    "id": "968b0104-ce75-4b9f-81c8-53dad0e63a44"
   },
   "source": [
    "**Use case #2:** Using a similar approach, we can preprocess the dataset so that all dates (yyyy-mm-dd) are replaced by their respective year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b04308d1-ec58-4fee-90b8-cb72e6ecdb79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "b04308d1-ec58-4fee-90b8-cb72e6ecdb79",
    "outputId": "c37a51d5-0655-464f-d4a8-b1ae9261a495"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_index</th>\n",
       "      <th>comment</th>\n",
       "      <th>match_start</th>\n",
       "      <th>match_end</th>\n",
       "      <th>comment_without_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155</td>\n",
       "      <td>I'll refer to [a post I made a while back](https://www.reddit.com/r/TUDelft/comments/sv34r7/are_...</td>\n",
       "      <td>1047</td>\n",
       "      <td>1057</td>\n",
       "      <td>I'll refer to [a post I made a while back](https://www.reddit.com/r/TUDelft/comments/sv34r7/are_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>323</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>323</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...</td>\n",
       "      <td>103</td>\n",
       "      <td>113</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>323</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...</td>\n",
       "      <td>534</td>\n",
       "      <td>544</td>\n",
       "      <td>I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_index  \\\n",
       "0            155   \n",
       "1            323   \n",
       "2            323   \n",
       "3            323   \n",
       "\n",
       "                                                                                               comment  \\\n",
       "0  I'll refer to [a post I made a while back](https://www.reddit.com/r/TUDelft/comments/sv34r7/are_...   \n",
       "1  I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...   \n",
       "2  I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...   \n",
       "3  I will be messaging you in 1 day on [**2022-11-09 20:20:41 UTC**](http://www.wolframalpha.com/in...   \n",
       "\n",
       "   match_start  match_end  \\\n",
       "0         1047       1057   \n",
       "1           39         49   \n",
       "2          103        113   \n",
       "3          534        544   \n",
       "\n",
       "                                                                                  comment_without_date  \n",
       "0  I'll refer to [a post I made a while back](https://www.reddit.com/r/TUDelft/comments/sv34r7/are_...  \n",
       "1  I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...  \n",
       "2  I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...  \n",
       "3  I will be messaging you in 1 day on [**2022 20:20:41 UTC**](http://www.wolframalpha.com/input/?i...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple regex to match the dates in the form of yyyy-mm-dd\n",
    "date_regex = \"\\d{4}-\\d{2}-\\d{2}\"\n",
    "\n",
    "# lets first extract all the matches with dates\n",
    "date_matches = search_all_pattern(date_regex, reddit_data)\n",
    "\n",
    "#We need a substitution function that extracts the year based on the input, as we dont want to simply have a specific year but be adaptable\n",
    "# Input: a date (in the form of yyyy-mm-dd)\n",
    "# Output: a string corresponding to the date's year\n",
    "def replace_year(date):\n",
    "    # Extract the year from the date ( first 4 characters) and return it\n",
    "    return date.group(0)[:4]\n",
    "\n",
    "# function to substitute the dates. here we use the re.sub and note that instead of providing a \"new_word\" we provide a function (replace_year)\n",
    "# that dictates what exactly should be the output based on the input\n",
    "# Input: a string corresponding to the Reddit comment\n",
    "# Output: a substituted comment where dates are replaced by their year\n",
    "def sub_dates(text):\n",
    "    return re.sub(date_regex, replace_year, text)\n",
    "\n",
    "# we use the pandas map function to run the function \"sub_dates\" to all the rows in our dataframe. we use as input the data in the \"comment\" column\n",
    "# we store the output in the \"comment_without_date\" column\n",
    "date_matches['comment_without_date'] = date_matches['comment'].map(sub_dates)\n",
    "\n",
    "# lets inspect manually the changes by comparing the \"comment\" and \"comment_without_date\" columns\n",
    "date_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149fda9-dbca-48ef-9247-f99f9499fa19",
   "metadata": {
    "id": "e149fda9-dbca-48ef-9247-f99f9499fa19"
   },
   "source": [
    "### **Exercise:** Using regular expressions, implement a function that removes all numbers from the comments included in the Reddit dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab32318-0e1a-468f-aaf7-c442feff2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d363a-1948-4c4d-bc87-859b22e38c4d",
   "metadata": {
    "id": "1a2d363a-1948-4c4d-bc87-859b22e38c4d"
   },
   "source": [
    "## 2. Word Tokenization & Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fdf3c-d9a4-4a51-8a5d-b32a3d9c0987",
   "metadata": {
    "id": "a95fdf3c-d9a4-4a51-8a5d-b32a3d9c0987"
   },
   "source": [
    "Word tokenization refers to the procedure of segmenting a string into words. Here, we will see how we can perform word tokenization using Python libraries.\n",
    "There are many libraries that have implemented many off-the-shelf word tokenization methods:\n",
    "1. NLTK\n",
    "2. Gensim library\n",
    "3. SpaCy\n",
    "4. Stanford CoreNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3226e9b3-7e96-43d6-94b3-4b6999605621",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3226e9b3-7e96-43d6-94b3-4b6999605621",
    "outputId": "da2d091b-51fb-42af-8625-e811b0b3cec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets see how a specific Reddit post will get tokenized using these Libraries...\n",
      "Raw post = Iâ€™d say they give that after jan 13 or when everyoneâ€™s place has been secured\n",
      "NLTK Tokenization = ['I', 'â€™', 'd', 'say', 'they', 'give', 'that', 'after', 'jan', '13', 'or', 'when', 'everyone', 'â€™', 's', 'place', 'has', 'been', 'secured'] Num of words = 19\n",
      "Gensim Tokenization = ['I', 'd', 'say', 'they', 'give', 'that', 'after', 'jan', 'or', 'when', 'everyone', 's', 'place', 'has', 'been', 'secured'] Num of words = 16\n",
      "SpaCy Tokenization = ['I', 'â€™d', 'say', 'they', 'give', 'that', 'after', 'jan', '13', 'or', 'when', 'everyone', 'â€™s', 'place', 'has', 'been', 'secured'] Num of words = 17\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.utils import tokenize\n",
    "import en_core_web_sm # python3 -m spacy download en_core_web_sm\n",
    "# We load an NLP model available on SpaCy (can be used for various tasks)\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "# function to perform simple word tokenization using the NLTK library\n",
    "# Input: a string of text\n",
    "# Output: a list of strings (each string corresponds to a word)\n",
    "def tokenize_text_nltk(text):\n",
    "    return word_tokenize(text) # we use the word_tokenize method from NLTK\n",
    "\n",
    "\n",
    "# function to perform simple word tokenization using the Gensim library\n",
    "# Input: a string of text\n",
    "# Output: a list of strings (each string corresponds to a word)\n",
    "def tokenize_text_gensim(text):\n",
    "    return list(tokenize(text)) # we use the tokenize method from Gensim\n",
    "\n",
    "# function to perform simple word tokenization using the SpaCy library\n",
    "# Input: a string of text\n",
    "# Output: a list of strings (each string corresponds to a word)\n",
    "def tokenize_text_spacy(text):\n",
    "    # we run the spacy model\n",
    "    doc = spacy_nlp(text)\n",
    "    # we extract the words after tokenization and return them\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# use the functions to tokenize an example of a post in our Reddit data\n",
    "tokenized_nltk = tokenize_text_nltk(reddit_data[0])\n",
    "tokenized_gensim = tokenize_text_gensim(reddit_data[0])\n",
    "tokenized_spacy = tokenize_text_spacy(reddit_data[0])\n",
    "\n",
    "# print the results to check how the text is tokenized\n",
    "print(\"Lets see how a specific Reddit post will get tokenized using these Libraries...\")\n",
    "print(\"Raw post = %s\" %(reddit_data[0]))\n",
    "print(\"NLTK Tokenization = %s Num of words = %d\" %(str(tokenized_nltk), len(tokenized_nltk)))\n",
    "print(\"Gensim Tokenization = %s Num of words = %d\" %(str(tokenized_gensim), len(tokenized_gensim)))\n",
    "print(\"SpaCy Tokenization = %s Num of words = %d\" %(str(tokenized_spacy), len(tokenized_spacy)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29825e3-3852-43de-aecf-d09fd2178100",
   "metadata": {
    "id": "a29825e3-3852-43de-aecf-d09fd2178100"
   },
   "source": [
    "### Examine the output of these three word tokenization methods. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91975e67-10e8-4f0c-bf18-943665431e60",
   "metadata": {
    "id": "91975e67-10e8-4f0c-bf18-943665431e60"
   },
   "source": [
    "Each word tokenization method deals with the problem of word tokenization slighly different. For instance, the Gensim implementation removes all punctuation and numbers before tokenizing the document. Also, the NLTK and SpaCy tokenization methods behave differently for words or parts of the document that include punctuation such as \"I'd\". Overall, depending on the intended use case and analysis, one should choose which of these methods better suits their needs and if none is well-suited for the intended use, then we implement our own word tokenization method (e.g., using regular expressions or word dictionaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb48b8-dc58-4962-b6c4-92666b4b1965",
   "metadata": {
    "id": "6eeb48b8-dc58-4962-b6c4-92666b4b1965"
   },
   "source": [
    "### Usually, word tokenization is used in conjuction with other basic text normalization operations such as stopword removal.\n",
    "\n",
    "Stop words are common words that do not carry much meaningful information and are often removed in pre-processing steps of text mining tasks. Here's how you can remove stop words using three popular libraries: NLTK, SpaCy, and Gensim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00dd7775-011a-41d1-a359-42ff592ba6cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00dd7775-011a-41d1-a359-42ff592ba6cc",
    "outputId": "3d91c5c0-2d7d-412a-8368-62e729f06652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# remove stopwords using NLTK\n",
    "# Input: a string of text\n",
    "# Output: a list of words corresponding to the input text without stop words\n",
    "def remove_stopwords_nltk(text):\n",
    "    # load the list of stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # split the text into word using the word tokenization function we implmentated before\n",
    "    word_tokens = tokenize_text_nltk(text)\n",
    "\n",
    "    # return the words as long as they are not in our stop words list\n",
    "    return [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "\n",
    "# remove stopwords using SpaCy\n",
    "# Input: a string of text\n",
    "# Output: a list of words corresponding to the input text without stop words\n",
    "def remove_stopwords_spacy(text):\n",
    "    # process the text using SpaCy\n",
    "    doc = spacy_nlp(text)\n",
    "\n",
    "    # return the words as long as they are not in SpaCy's stopword list\n",
    "    return [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# remove stopwords using Gensim's method\n",
    "# Input: a string of text\n",
    "# Output: a string without any stop words\n",
    "def remove_stopwords_gensim(text):\n",
    "    return tokenize_text_nltk(remove_stopwords(text))\n",
    "\n",
    "toy_example = \"This is a sample sentence, showing off the stop words removal.\"\n",
    "\n",
    "print(remove_stopwords_nltk(toy_example))\n",
    "print(remove_stopwords_spacy(toy_example))\n",
    "print(remove_stopwords_gensim(toy_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f02d2-7c9d-47aa-a7b4-6e108dd5e010",
   "metadata": {
    "id": "f40f02d2-7c9d-47aa-a7b4-6e108dd5e010"
   },
   "source": [
    "What do you observe from all these various methods to perform stop word removal? What is the effect of the case of the text? Play with some toy examples to see what is the result for various instances of the same word (e.g., This vs this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e0ac6-b190-4af9-a801-d508ad10caab",
   "metadata": {
    "id": "da2e0ac6-b190-4af9-a801-d508ad10caab"
   },
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0db2cc-4207-45eb-af89-dd362e2c7cfa",
   "metadata": {
    "id": "de0db2cc-4207-45eb-af89-dd362e2c7cfa"
   },
   "source": [
    "Sentence segmentation is the problem of dividing a string of written language into its component sentences. The idea here seems very trivial, and it is a trivial task for a human since we understand the structure of each sentence. However, for a machine, this task is not straightforward. Here, we are gonna see how we can use the NLTK and SpaCy libraries to perform sentence segmentation.\n",
    "\n",
    "We do not consider Gensim here because Gensim's *split_sentences* is a rather basic and crude method of sentence segmentation. It essentially just splits on periods, without more advanced logic for dealing with periods in the middle of sentences (like abbreviations or decimal numbers). This is a naive approach and will fail in cases where periods are used in floating numbers or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f04adf9-0ef2-46fa-8eaa-a25cee0c9f23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f04adf9-0ef2-46fa-8eaa-a25cee0c9f23",
    "outputId": "3f7e3462-6921-4610-8b21-66158da5821c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Smith graduated from the University of Amsterdam.',\n",
       " 'His Ph.D. thesis was inspiring.',\n",
       " 'He now works at OpenAI Corp. and earns $1200.50 per day.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# function to perform sentence segmentation using the NLTK library\n",
    "# Input: a string of text\n",
    "# Output: a list of strings (each string corresponds to a sentence)\n",
    "def sentence_segm_nltk(text):\n",
    "    return sent_tokenize(text) # we use the sent_tokenize method from NLTK\n",
    "\n",
    "# function to perform simple sentence segmentation using the SpaCy library\n",
    "# Input: a string of text\n",
    "# Output: a list of strings (each string corresponds to a sentence)\n",
    "def sentence_segm_spacy(text):\n",
    "    # we run the spacy model\n",
    "    doc = spacy_nlp(text)\n",
    "    # we extract the words after tokenization and return them\n",
    "    return [sent for sent in doc.sents]\n",
    "\n",
    "# lets try the NLTK Sentence segmentation method using a toy example\n",
    "toy_example = \"Dr. Smith graduated from the University of Amsterdam. His Ph.D. thesis was inspiring. He now works at OpenAI Corp. and earns $1200.50 per day.\"\n",
    "sentence_segm_nltk(toy_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32ce8b1d-b000-42c5-96f0-9532d97422a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32ce8b1d-b000-42c5-96f0-9532d97422a5",
    "outputId": "7078ca58-35da-4310-ce10-35204846021f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dr. Smith graduated from the University of Amsterdam.,\n",
       " His Ph.D. thesis was inspiring.,\n",
       " He now works at OpenAI Corp. and earns $1200.50 per day.]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try the SpaCy Sentence segmentation method using a toy example\n",
    "sentence_segm_spacy(toy_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc72bd5-87e0-4e07-8bef-6ae3081fa6c8",
   "metadata": {
    "id": "5fc72bd5-87e0-4e07-8bef-6ae3081fa6c8"
   },
   "source": [
    "## 3. Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404951f4-bc25-4b1f-9d15-08dbdb1eb679",
   "metadata": {
    "id": "404951f4-bc25-4b1f-9d15-08dbdb1eb679"
   },
   "source": [
    "In natural language processing, **stemming** and **lemmatization** are techniques for reducing words to their root form. They are used in text preprocessing steps to group words having the same meaning but different representations.\n",
    "\n",
    "* **Stemming:** It reduces the word to its stem or root form. For example, 'jumps', 'jumped', and 'jumping' are stemmed to 'jump'. It is a rudimentary rule-based process of stripping the suffixes (\"ing\", \"ly\", \"es\", \"s\" etc) from a word.\n",
    "\n",
    "* **Lemmatization:** It reduces the word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, which may result in incorrect meanings and spelling errors.\n",
    "\n",
    "\n",
    "Here, we are going to see how we can implement in Python, lemmatization and stemming using the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa416d21-d11a-4d0d-aedc-16b3871a196d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa416d21-d11a-4d0d-aedc-16b3871a196d",
    "outputId": "8e389270-51e6-4d2d-8feb-2fdf4e38739b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', 'â€™', 's', 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'things-nam', 'and', 'height', 'and', 'soundings-with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# function that performs stemming using the Porter Stemmer algorithm\n",
    "# Input: a string of text\n",
    "# Output: a list of stemmed words extracted after performing tokenization and stemming using the Porter Stemmer Algorithm\n",
    "def stem_text(text):\n",
    "    # create an instance of PorterStemmer (simple stemming algorithm)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # split the text into words using our already defined word tokenization methods (here we use NLTK's tokenization)\n",
    "    words = tokenize_text_nltk(text)\n",
    "\n",
    "    # stem each word using our PorterStemmer instance\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # return the list of the stemmed words\n",
    "    return stemmed_words\n",
    "\n",
    "# stem an example document\n",
    "toy_document = \"This was not the map we found in Billy Bonesâ€™s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.\"\n",
    "print(stem_text(toy_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c86c6073-672d-494c-afae-e1ca181b54f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c86c6073-672d-494c-afae-e1ca181b54f3",
    "outputId": "fc933aab-363b-44b9-cf65-026e0980f8d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', 'â€™', 's', 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things-names', 'and', 'height', 'and', 'soundings-with', 'the', 'single', 'exception', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# function that performs lemmatization using the WordNet model available via NLTK\n",
    "# Input: a string of text\n",
    "# Output: a list of lemmatized words extracted after performing tokenization and lemmatization using the Porter Stemmer Algorithm\n",
    "def lemmatize_text(text):\n",
    "    # create an instance of the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # split the text into words using our already defined word tokenization methods (here we use NLTK's tokenization)\n",
    "    words = tokenize_text_nltk(text)\n",
    "\n",
    "    # lemmatize each word using our WordNetLemmatizer instance\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # return the list of the lemmatized words\n",
    "    return lemmatized_words\n",
    "\n",
    "print(lemmatize_text(toy_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00af561-f0fe-467c-9569-f6301b702727",
   "metadata": {
    "id": "f00af561-f0fe-467c-9569-f6301b702727"
   },
   "source": [
    "## 4. Basic text similarity metrics based on Hamming Distance, Levenshtein Distance, and Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54fe650a-865b-4ec9-a215-ff5e199aab52",
   "metadata": {
    "id": "54fe650a-865b-4ec9-a215-ff5e199aab52"
   },
   "outputs": [],
   "source": [
    "# function to calculate the hamming distance between two strings\n",
    "# recall that the hamming distance is simply the number of characters that mismatch\n",
    "# Recall that the Hamming distances works only for strings that have equal length\n",
    "# Input: two strings\n",
    "# Output: an integer corresponding to the Hamming distance between the two strings\n",
    "def hamming_distance(string1, string2):\n",
    "    # check to ensure that the two strings have the same length. raise an error if they do not have the same length\n",
    "    if len(string1) != len(string2):\n",
    "        raise ValueError(\"Undefined for sequences of unequal length.\")\n",
    "\n",
    "    return sum(el1 != el2 for el1, el2 in zip(string1, string2))\n",
    "\n",
    "\n",
    "# function that calculates the jaccard index (score) between two sets of words\n",
    "# recall that the jaccard index is the length of the intersection divided by the length of the union of the two sets\n",
    "# Input: two sets corresponding to words\n",
    "# Ouput: a score between 0 and 1 corresponding to the Jaccard index of the two sets\n",
    "def jaccard_index(set1, set2):\n",
    "\n",
    "    # calculate the length of the intersection of the two sets\n",
    "    intersection = len(set(set1).intersection(set(set2)))\n",
    "\n",
    "    # calculate the length of the union between the two sets\n",
    "    union = len(set(set1)) + len(set(set2)) - intersection\n",
    "\n",
    "    # calculate jaccard score and return it\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# function that calculates the Levenshtein Distance by creating the Levenshtein Distance matrix\n",
    "# Input: two strings\n",
    "# Output: an integer corresponding to the Levenshtein Distance of the two strings\n",
    "def levenshtein_distance(string1, string2):\n",
    "\n",
    "    # define the number of rows and columns in our table (length of string + 1)\n",
    "    size_x = len(string1) + 1\n",
    "    size_y = len(string2) + 1\n",
    "\n",
    "    # create the matrix (list of lists)\n",
    "    matrix = [[0] * size_y for _ in range(size_x)]\n",
    "\n",
    "    # initiliaze the first row (with 0 to size_x)\n",
    "    for x in range(size_x):\n",
    "        matrix[x][0] = x\n",
    "\n",
    "    # initialize the first column (with 0 to size_y)\n",
    "    for y in range(size_y):\n",
    "        matrix[0][y] = y\n",
    "\n",
    "    # we iterate all indices of the table and we fill the distance matrix\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "\n",
    "            # if the characters match\n",
    "            if string1[x-1] == string2[y-1]:\n",
    "                # we select the minimum value between the insert/copy/delete operations\n",
    "                #Note: since the characters match this is equivalant to simply using the copy operation\n",
    "                # i.e., matrix[x][y] = matrix[x-1][y-1]\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x-1][y] + 1, # insert operation\n",
    "                    matrix[x-1][y-1], # copy operation\n",
    "                    matrix[x][y-1] + 1 # delete operation\n",
    "                )\n",
    "\n",
    "            # if the characters do not match\n",
    "            else:\n",
    "                # we select the minimum value between the insert/substitute/delete operations\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x-1][y] + 1, # insert operation\n",
    "                    matrix[x-1][y-1] + 1, # substitute operation\n",
    "                    matrix[x][y-1] + 1 # delete operation\n",
    "                )\n",
    "    # return the bottom right element in our matrix that corresponds to the Levenshtein Distance of the two strings\n",
    "    return matrix[size_x - 1][size_y - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e472ebcd-0234-483d-87d3-86e4d523a8f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e472ebcd-0234-483d-87d3-86e4d523a8f9",
    "outputId": "c1f2cec1-f57b-4829-e8d9-3be536f0a4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Distance between compture and computer = 4\n",
      "Levenshtein Distance between Saturday and Sunday = 3\n",
      "The Jaccard index between ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'] and ['the', 'lazy', 'dog', 'is', 'jumped', 'over', 'by', 'the', 'quick', 'brown', 'fox'] = 0.64\n"
     ]
    }
   ],
   "source": [
    "# lets test these distance metrics with toy examples\n",
    "doc1 = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "doc2 = [\"the\", \"lazy\", \"dog\", \"is\", \"jumped\", \"over\", \"by\", \"the\", \"quick\", \"brown\", \"fox\"]\n",
    "\n",
    "\n",
    "print(\"Hamming Distance between compture and computer = %d\" %(hamming_distance(\"compture\", \"computer\")))\n",
    "print(\"Levenshtein Distance between Saturday and Sunday = %d\" %(levenshtein_distance(\"Saturday\", \"Sunday\")))\n",
    "print(\"The Jaccard index between %s and %s = %.2f\" %(str(doc1), str(doc2), jaccard_index(doc1, doc2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8b592-6fee-4054-8350-9fba025fa0a8",
   "metadata": {
    "id": "b2b8b592-6fee-4054-8350-9fba025fa0a8"
   },
   "source": [
    "## **Use case:** Finding similar documents in a dataset\n",
    "\n",
    "Using the Reddit dataset, and a specific document, lets try to find the closest post in our dataset using one of the similarity metrics (for the purposes of this use case lets use Jaccard Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "862ee37d-096b-4dc9-98ae-c4fc9270ef48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "862ee37d-096b-4dc9-98ae-c4fc9270ef48",
    "outputId": "bb13da4c-d228-4cc0-a105-14ac421bba85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The pre university calculus and physics moocs by tudelft are a great resource to practise the concepts needed for the entrance exam.',\n",
       " 0.36363636363636365)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to find the most similar document in a corpus given a target document\n",
    "# Input: a string corresponding to a target document and a list of strings in our dataset to be used for finding the most similar document\n",
    "# Output: a string that have the highest jaccard index across all documents and its jaccard index score\n",
    "def find_closest_document(document, documents):\n",
    "\n",
    "    # Tokenize the document so that we have a list of words. For this use case we use the SpaCy tokenization defined above\n",
    "    words_in_document = tokenize_text_spacy(document.lower())\n",
    "\n",
    "    # initialize the closest_document variable and the highest jaccard index (we set it to something very low)\n",
    "    closest_document = \"\"\n",
    "    highest_jaccard_index = -1\n",
    "\n",
    "    # iterate through the documents we want to search in\n",
    "    for doc in documents:\n",
    "\n",
    "        # tokenize each document (.lower ensures that everything is in lowercase)\n",
    "        words_in_doc = tokenize_text_spacy(doc.lower())\n",
    "\n",
    "        # calculate jaccard index of this specific document and the document we are searching with\n",
    "        index = jaccard_index(words_in_document, words_in_doc)\n",
    "\n",
    "        # if its higher that the already existing highest_jaccard_index, update the variables holding the highest jaccard index and the closest document\n",
    "        if index > highest_jaccard_index:\n",
    "            highest_jaccard_index = index\n",
    "            closest_document = doc\n",
    "\n",
    "    # return the closest document based on the jaccard index\n",
    "    return closest_document, highest_jaccard_index\n",
    "\n",
    "# run the function with a test document\n",
    "find_closest_document(\"The university moocs are a good resource for the exam\", reddit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82db76-229b-4452-8276-a151e036aca4",
   "metadata": {
    "id": "2f82db76-229b-4452-8276-a151e036aca4"
   },
   "source": [
    "## **Exercise:** Finding similar documents in a dataset 2\n",
    "\n",
    "Implement and run a function called \"find_closest_document_levenshtein\" that finds the closest post in our dataset using the Levensthein Distance metric implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23764b6f-5e3b-4e17-b7ab-d8c26280c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479da7e6-2284-4f53-94df-c617d82d99e1",
   "metadata": {
    "id": "479da7e6-2284-4f53-94df-c617d82d99e1"
   },
   "source": [
    "Here, re-run the find_closest methods with various examples of documents to observe the results and assess how the following aspects affect the results:\n",
    "1. Inclusion of stop words in the data\n",
    "2. Length of the documents and how this affect the distance metrics and the Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047f122-6982-434f-9c60-ace2a00ce1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f1a78-bdf8-4ab1-8d08-9ce51d86ecf9",
   "metadata": {
    "id": "410f1a78-bdf8-4ab1-8d08-9ce51d86ecf9"
   },
   "source": [
    "## TI3160TU: Natural Language Processing - Basic Text Preprocessing Lab -- END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
