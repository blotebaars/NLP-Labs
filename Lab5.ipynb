{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aeccf74-c687-4a33-a8c0-0525ca723d3a",
   "metadata": {},
   "source": [
    "## TI3160TU: Natural Language Processing - Vector Semantics and Word Embeddings Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea0aff-44f2-4f23-a8b3-72aa783b11bd",
   "metadata": {},
   "source": [
    "In this hands-on lab, we will explore methods that allow us to assess the similarity of documents/words in a corpus. We are going to focus on three approaches/methods: \n",
    "\n",
    "1. **Term Frequency - Inverse Document Frequency (TF-IDF):** A measure that reflects how important a word is to a document in a corpus. The TF-IDF value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus, thus helping to adjust for the fact that some words appear more frequently in general.\n",
    "2. **Pointwise Mutual Information (PMI):** A metric that measures the association between two words. High PMI values indicate that the co-occurrence of two words is more than what would be expected if they were independent. \n",
    "3. **Word2vec models:** A popular neural network model for learning word embeddings in large texts. It captures the semantic meaning of words by representing them as vectors in a high-dimensional space. Words with similar meanings tend to be closer in this space, making it useful for tasks like word similarity and analogy.\n",
    "\n",
    "To demonstrate these methods, we are going to use a dataset that includes all tweets made by Donald Trump (available at: https://drive.google.com/file/d/1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6/view?usp=sharing). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07f722-1c7e-4405-a9c9-482a0d72fcd6",
   "metadata": {},
   "source": [
    "### 0. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955ccc3f-8922-43e1-be7d-e81dd60334fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                           98454970654916608\n",
       "text         Republicans and Democrats have both created ou...\n",
       "isRetweet                                                    f\n",
       "isDeleted                                                    f\n",
       "device                                               TweetDeck\n",
       "favorites                                                   49\n",
       "retweets                                                   255\n",
       "date                                       2011-08-02 18:07:48\n",
       "isFlagged                                                    f\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the dataset is in a csv file so lets use Panda's built in function to load a csv into a dataframe\n",
    "tweetsdf = pd.read_csv('tweets_01-08-2021.csv') # load the tweets using Pandas\n",
    "tweetsdf.iloc[0] # lets see what its included in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6a931-5eac-445f-b654-4248dd4f09db",
   "metadata": {},
   "source": [
    "There are a bunch of metadata included in the dataset. Here we are interested in the \"text\" field which corresponds to the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cf971b-901a-40a2-b083-3597c5b6f3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fd0ea-d392-4853-96a2-a0fcc0128fdd",
   "metadata": {},
   "source": [
    "The dataset includes data for 56,571 tweets. So our corpus has 56K documents in it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9dd74-53aa-4c08-9d2d-4eed9de3e41b",
   "metadata": {},
   "source": [
    "### 1. Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37043ee7-e497-4dcb-9609-24aad45c0406",
   "metadata": {},
   "source": [
    "To convert our documents to a matrix of TF-IDF features we are going to use the readily available **TfidfVectorizer** function from Sklearn. The most important parameters of this function are:\n",
    "1. *lowercase:* A boolean that indicates if the documents should be converted to lowercase. By defaults its true\n",
    "2. *max_df:* When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. Default value is 1.0.\n",
    "3. *min_df:* When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. Default value is 1\n",
    "3. *max_features:* If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Otherwise, all features are used. Default value is None\n",
    "4. *stop_words:* You can pass a string or a list of words that will be considered stopwords and will be removed from the documents. Default value is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ae1194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 58705)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the function to extract TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# define the behavior of our TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', # remove english stopwords\n",
    "                                  max_df=1.0, # essentially does not remove any feature\n",
    "                                  min_df=1, # essentially does not remove any feature\n",
    "                                  max_features=None) #essentially does not remove any feature\n",
    "\n",
    "# Compute TF-IDF vectors for each tweet\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tweetsdf['text'].tolist())\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ad828-bcdf-4e5a-a2e7-94d707c66ff5",
   "metadata": {},
   "source": [
    "Now we have converted our raw corpus to a TF-IDF matrix. Each row corresponds to a tweet and each column correspond to a TF-IDF feature, which in this case corresponds to a word that exists in the vocabulary that is created from the words that appear in our dataset. As we can see from the matrix's shape, our TF-IDF representations include a very large number of features, in this case 58K features (see #columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5a746-dad5-4aa2-89b9-35879aa54df8",
   "metadata": {},
   "source": [
    "To identify what each feature corresponds to, we can access the vocabulary of our TfidfVectorizer. It includes the mapping between the word and the feature index in our matrix. Lets print some of the words in our vocabulary and their corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c13305-8b64-4750-b9f4-0f9178435fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'republicans': 42983, 'democrats': 14724, 'created': 13156, 'economic': 16919, 'problems': 40394}\n"
     ]
    }
   ],
   "source": [
    "# how to know what each column corresponds to\n",
    "feature_names = tfidf_vectorizer.vocabulary_\n",
    "print(dict(list(feature_names.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33a9ea-50c0-4f9a-97b3-365dc36fb39b",
   "metadata": {},
   "source": [
    "This means that column 42983 corresponds to the word \"republicans\", column 14724 corresponds to the word \"democrats\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24e760-e793-43b3-828e-e23fe33bd6e6",
   "metadata": {},
   "source": [
    "Usually, we do not use so large number of features so we need to reduce the number of features (i.e., the dimensionality of this matrix). Many features are not very informative and can be excluded. There are three ways to reduce the number of features in Sklearn's TF-IDF implementation:\n",
    "1. **Change max_df parameter:** The max_df parameter in Sklearn's TF-IDF Vectorizer refers to the maximum document frequency. It's used to filter out terms that have a document frequency strictly higher than the given threshold. This can be in terms of a proportion (0.0-1.0) or an absolute number. For instance, setting max_df to 0.8 would filter out terms that appear in 80% of the documents. This is useful for excluding terms that are too common and are unlikely to carry much meaning.\n",
    "2. **Change min_df parameter:** The min_df parameter refers to the minimum document frequency. Words that have a document frequency strictly lower than the given threshold will be ignored. Like max_df, this threshold can be a proportion or an absolute count. Setting min_df to 0.1, for example, would exclude words that appear in less than 10% of the documents. This is helpful for filtering out rare terms that might be typos or are too infrequent to analyze.\n",
    "3. **Set max_features parameter:** This parameter is straightforward. It sets the maximum number of features (i.e., unique terms or words) to be retained. When building the vocabulary, it only considers the top max_features ordered by term frequency across the corpus. For example, setting max_features to 1000 would only use the 1000 terms that appear most frequently across the dataset. It's a way to limit the dimensionality of the output and can be useful when you have computational constraints or when working with a very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff26e9df-ea4f-497c-a1f4-10bd57b86ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 9703)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see an example with setting the max_df and min_df parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', \n",
    "    max_df=0.8,        # Ignore terms that appear in more than 80% of the documents\n",
    "    min_df=5,       # Ignore terms that appear in less than 5 documents\n",
    ")\n",
    "\n",
    "tfidf_matrix2 = tfidf_vectorizer.fit_transform(tweetsdf['text'].tolist())\n",
    "tfidf_matrix2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d3f87-ade2-4691-8ea4-58241204783b",
   "metadata": {},
   "source": [
    "Here, we observe that by setting the max_df to 0.8 and min_df to 5, we reduced the number of features and the dimensionality of the matrix from 58K to 9.7K. If, we want to further reduce, we can tweak these parameters or set only the max_features parameter. For instance, if we want to extract a TF-IDF matrix with max 5000 features we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace19b37-9f43-4c91-9980-5362510b6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see an example with setting the max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', \n",
    "    max_features=5000 # Consider only the top 5000 terms by term frequency\n",
    ")\n",
    "\n",
    "tfidf_matrix3 = tfidf_vectorizer.fit_transform(tweetsdf['text'].tolist())\n",
    "tfidf_matrix3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebe2cb-4922-434e-bde6-8f95e40cac1d",
   "metadata": {},
   "source": [
    "We confirm the number of features by inspecting the shape of our matrix. Now, having converted our corpus into TF-IDF representations we can use them for various NLP tasks like performing classification tasks, assessing the similarity between documents, clustering the documents to understand the main topics of discussion, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe7c94-ad5a-4029-bc7c-4c981910a534",
   "metadata": {},
   "source": [
    "### 1.1 Using TF-IDF representations to assess document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca0dff-2c64-4ecb-8d73-0f0985279f26",
   "metadata": {},
   "source": [
    "Here, we will demonstrate how we can use our TF-IDF representations to assess similarity between documents. We will follow the next steps:\n",
    "1. We provide an example document (query_document) for which we aim to find the document that is more similar in our corpus.\n",
    "2. We convert the query document to a TF-IDF representation similarly to what we did before\n",
    "3. We calculate the cosine similarities between our query TF-IDF representation and all TF-IDF representations in our corpus\n",
    "4. We find the document in our corpus that has the highest cosine similarity with the query document and we print it along with the cosine similarit\n",
    "\n",
    "Lets see how we can implement these steps using Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4146a5fe-b621-4c5f-89e0-204e03efd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar document to 'Hillary Clinton is a crooked person' is: 'Crooked Hillary Clinton, perhaps the most dishonest person to have ever run for the presidency, is also one of the all time great enablers!'\n",
      "\n",
      "Cosine Similarity of these two documents is: '0.6787896811118255'\n"
     ]
    }
   ],
   "source": [
    "# import the method from sklearn that allow us to calculate all pairwise cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# here we have a test document. we want to find the tweet from Donald Trump that is more similar to this document\n",
    "query_document = \"Hillary Clinton is a crooked person\"\n",
    "\n",
    "# first we need to convert our query document to a TF-IDF representation like before\n",
    "query_tfidf = tfidf_vectorizer.transform([query_document])\n",
    "\n",
    "# we calculate all cosine similarities between the query TF-IDF representation and all documents in our corpus (their TF-IDF representations)\n",
    "cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix3).flatten()\n",
    "\n",
    "# find the index of the document that has the highest cosine similarity\n",
    "most_similar_doc_idx = cosine_similarities.argmax()\n",
    "\n",
    "# print the results\n",
    "print(f\"The most similar document to '{query_document}' is: '{tweetsdf['text'].tolist()[most_similar_doc_idx]}'\\n\")\n",
    "print(f\"Cosine Similarity of these two documents is: '{cosine_similarities[most_similar_doc_idx]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9337e-010f-4329-933a-1981c732e282",
   "metadata": {},
   "source": [
    "### 1.2 Using TF-IDF representations to make exploratory analysis (e.g., by clustering the representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370b9a7-6bdc-43e1-8b10-12cee78f5213",
   "metadata": {},
   "source": [
    "Instead of assessing the similarity of a single query document to our corpus, we can analyze our corpus in an exploratory manner by creating clusters of documents that are semantically similar based on their TF-IDF representations. To do this, we are going to follow the next steps:\n",
    "\n",
    "1. First, sample our corpus to reduce its size (usually we cluster the entire dataset but for purposes of this demo we will sample randomly 5K documents to speed up the process of clustering).\n",
    "2. We will a use a standard clustering algorithm (DBSCAN) to group the documents into clusters based on their cosine similarities of the TF-IDF representations.\n",
    "3. Print a maximum of 10 documents for each cluster (except cluster -1, which is the noise cluster that includes documents that are not clustered with any other documents).\n",
    "\n",
    "Lets see how we can implement these steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841118f1-ff02-497f-b52f-105b93850c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      "----------------------------------------\n",
      "\"\"\"@PlaisanceAnn: @realDonaldTrump Trump for President!!!\"\"\"\n",
      "@scottlara1961 Have a Happy Birthday.\n",
      "WITCH HUNT!\n",
      "https://t.co/WjbNIvW96r\n",
      "https://t.co/Mn4EJ7Jbh8\n",
      "#TrumpAdvice http://t.co/f0Sjhe80OL\n",
      "\"\"\"@BobMenziesGolf: @realDonaldTrump @TylerClodfelter It should be locker #1, in my opinion! #Trump2016\"\"\"\n",
      "https://t.co/2PoLkxQ8KE\n",
      "Thank you for all of your support! Let's #MakeAmericaGreatAgain! #Trump2016 https://t.co/G7o6b0cfZz\n",
      "https://t.co/P46CNUBicX\n",
      "\n",
      "Cluster 4:\n",
      "----------------------------------------\n",
      "@sdoocy @BroRay You always look great on TV, even without a Trump tie!\n",
      "\"\"\"@JVince81: @bforrealstevens  GovChristie @realDonaldTrump That tie is awesome. I need that tie. Where can I get it?\"\"  MACY'S\"\n",
      "\"\"\"@benwiley3: @realDonaldTrump @Macys my Donald Trump tie is my prized possession in my closet!\"\"  Great!\"\n",
      "\"\"\"@Jesus_Mohammad: @realDonaldTrump should I get the red Donald trump tie or the blue Donald trump tie???\"\" Get them both at Macy's-thanks!\"\n",
      "\n",
      "Cluster 1:\n",
      "----------------------------------------\n",
      "\"\"\"@JHasseyjr: \"\"@abunnieslife: Mr Trump we had the honor of visiting your beautiful tower in Chicago You own a grand building sir! Thank\"\"\"\n",
      "\"\"\"@JacobsenSusan: @realDonaldTrump LOVE Trump Tower in Chicago!! Beautiful building!!\"\"\"\n",
      "\"\"\"@CJohnson623: @realDonaldTrump I love the Trump building in Chicago\"\"\"\n",
      "\"\"\"@AveGratiaPlena: @realDonaldTrump I saw Trump Tower in Chicago on Fox...it looks beautiful! Congratulations! Love the TRUMP sign too.\"\"\"\n",
      "\"\"\"@Lizzieliz__: #Chicago #Trumptower @realDonaldTrump http://t.co/afES4CxrPg\"\" Great building.\"\n",
      "\"\"\"@cotyybaby: Why the fuss? Trump has the most beautiful building in Chicago! That sign will be iconic #TRUMP #Elegance #TrumpChicago\"\"\"\n",
      "\n",
      "Cluster 2:\n",
      "----------------------------------------\n",
      ".@JuddApatow I agree!\n",
      "I agree 100%. https://t.co/9lgiXcdlC5\n",
      "\"\"\"@RealTerryPatton: @realDonaldTrump @JenCromartie Do it Mr. Trump, we are screwed if you don't.\"\"  I agree!\"\n",
      "\"\"\"@siddiquehash @realDonaldTrump @krauthammer I Agree Donald!\"\"\"\n",
      "\"\"\"@Patrick92299179: @realDonaldTrump @oreillyfactor Couldn't agree more!\"\"\"\n",
      "\n",
      "Cluster 3:\n",
      "----------------------------------------\n",
      "@RebaForever58   Yes.\n",
      "@jamezypell   Yes!\n",
      "\"\"\"@Prayformetoo: @realDonaldTrump @RyanKainec1 Yes, and what a great first lady we would have!  True!\"\n",
      "\"\"\"@KEEMSTARx: Mexican criminals coming into USA? YES Did Trump say we need to stop them? YES Did he say Mexicans are good? YES Racism? NO\"\"\"\n",
      "\"\"\"@thecarlossegura: @realDonaldTrump do u like Lettermans show?\"\"  Yes, much smarter than Leno.\"\n",
      "#CelebApprentice Selfies- yes or no?\n",
      "@omz213  Yes!\n",
      "@Skyhawk442  Yes, and I am Presbyterian.\n",
      "\n",
      "Cluster 5:\n",
      "----------------------------------------\n",
      "RT @realDonaldTrump: FAKE NEWS IS THE ENEMY OF THE PEOPLE!\n",
      "Will be back on the Campaign Trail soon!!! The Fake News only shows the Fake Polls.\n",
      "More Fake News! https://t.co/URewvfAUDl\n",
      "They are Fake News Losers! https://t.co/3RHcBZogms\n",
      "The greatest of all time. Fake News! https://t.co/jiWjLrynQW\n",
      "FAKE NEWS! https://t.co/Y8RwjmJ9D1\n",
      "This case had nothing to do with me. Fake News (as usual!). https://t.co/CZnO4GFcco\n",
      "\n",
      "Cluster 6:\n",
      "----------------------------------------\n",
      "President Trump Approval Rating in the Republican Party at 96%. Thank You!\n",
      "96% Approval Rating in the Republican Party. Thank you!\n",
      "95% Approval Rating in the Republican Party, a Record! 53% overall (plus add 9 points?). Corrupt Democrat politicians have brought me to highest polling numbers ever with the Impeachment Hoax. Thank you Nancy!\n",
      "95% Approval Rating in the Republican Party, a Record. 218 Federal Judges, also a Record. 2 Supreme Court Justices. Thank you!\n",
      "My Approval Rating in the Republican Party is 95%, a Record. Thank you! #2020Election\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# text of our tweets in the dataset\n",
    "tweets = tweetsdf['text'].tolist()\n",
    "\n",
    "# Randomly sample 5000 rows from the tfidf_matrix\n",
    "random_indices = np.random.choice(tfidf_matrix3.shape[0], 5000, replace=False)\n",
    "sampled_tfidf = tfidf_matrix3[random_indices]\n",
    "\n",
    "# cluster the sampled TF-IDF representations\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n",
    "clusters = dbscan.fit_predict(sampled_tfidf)\n",
    "\n",
    "# find the cluster of each document\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Filter and print the tweets that are not in the noise cluster\n",
    "clusters = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    if label != -1:\n",
    "        clusters.setdefault(label, []).append(tweets[random_indices[idx]])\n",
    "\n",
    "for cluster_id, cluster_tweets in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\\n{'-'*40}\")\n",
    "    for tweet in cluster_tweets[:10]:\n",
    "        print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05d0b8-b571-446a-a6ee-ddb5f5b43918",
   "metadata": {},
   "source": [
    "### **Exercise: Use TF-IDF representations for classification tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe31be-e2d0-4500-8edb-958266a99f65",
   "metadata": {},
   "source": [
    "Using TF-IDF representations for classification tasks is a common approach. In this exercise, you are asked to perform a classification task using TF-IDF. We are going to re-use the dataset that we used for the Classification tasks lab, which includes a dataset of movie reviews and their sentiment. The steps are the following:\n",
    "\n",
    "1. Load the IMDB reviews dataset that is a labeled dataset including movie reviews and their sentiment\n",
    "2. Vectorize the reviews: Convert the reviews into a matrix of TF-IDF features.\n",
    "3. Train-Test Split: Split your data into a training set and a test set.\n",
    "4. Train a Classifier: Use the training data to train a logistic regression classifier.\n",
    "5. Evaluate the Classifier: Test the classifier on the test set to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96edd9ee-7024-4ec5-89b6-6b8f8d725114",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d9e61-8895-4d12-92e7-8b92117540f6",
   "metadata": {},
   "source": [
    "### 2. Pointwise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacaeba-2264-4fc4-99fd-dba4b01aef8c",
   "metadata": {},
   "source": [
    "Pointwise Mutual Information (PMI) is a measure used in natural language processing and text mining. PMI is frequently used to identify strong associations between words in large text corpora.\n",
    "\n",
    "## Formula\n",
    "\n",
    "Given two words \\(x\\) and \\(y\\), the PMI between them is defined as:\n",
    "\n",
    "$\\text{PMI}(x, y) = \\log \\left( \\frac{p(x, y)}{p(x)p(y)} \\right)$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $p(x, y)$ is the joint probability of \\(x\\) and \\(y\\) occurring together.\n",
    "- $p(x)$ and $p(y)$ are the probabilities of \\(x\\) and \\(y\\) occurring independently.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "If \\(x\\) and \\(y\\) are independent, their PMI will be 0. If they co-occur more frequently than expected under independence, PMI will be positive; if less frequently, it will be negative.\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "To calculate PMI, first, we are gonna preprocess the corpus (remove stopwords, tokenize, etc.), then calculate occurrence and co-occurrence of words, and then use the above formula to calculate the PMI between two words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff9cdf-37aa-4151-a863-5a5ec30cac2b",
   "metadata": {},
   "source": [
    "#### 2.1 Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fea87-9db0-411f-be7d-0fbef7f197de",
   "metadata": {},
   "source": [
    "We make some standard pre-processing on the corpus by removing special characters, URLs, making everything lowercase, and tokenizing the documents into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8edf255b-e7aa-4eb9-892e-95c343575979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "import math \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# our corpus\n",
    "docs = tweetsdf['text'].tolist()\n",
    "\n",
    "# pre-process each document \n",
    "preprocessed_docs = []\n",
    "for doc in docs:\n",
    "    res = doc.lower() # lowercase\n",
    "    res = re.sub(r'[^a-zA-Z ]', '', res) # remove numbers/punctuation\n",
    "    res = re.sub(r'http\\S+', '', res) # remove URLs \n",
    "    words = []\n",
    "    for word in res.split(' '): \n",
    "        if len(word)>0 and word not in stop_words:\n",
    "            words.append(word)\n",
    "    preprocessed_docs.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e28281-d26d-4aef-b16c-5eed779ee991",
   "metadata": {},
   "source": [
    "#### 2.2 Calculate Frequencies of Words and Co-occurrence of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b70fceb-750a-472b-a4b4-e2af3ab454d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate word frequencies and word co-occurrences on a corpus\n",
    "# INPUT: Tokenized corpus\n",
    "# OUTPUT: Two dictionaries that include word frequencies and word co-occurences in the corpus\n",
    "def calculate_frequencies(tokenized_corpus):\n",
    "    # dictionaries to hold the word frequencies and word co-occurrences\n",
    "    word_freq = defaultdict(int)\n",
    "    cooccur_freq = defaultdict(int)\n",
    "    \n",
    "    # calculate word frequencies\n",
    "    for doc in tokenized_corpus:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            word_freq[word] += 1\n",
    "            \n",
    "        # calculate co-occurrence of words\n",
    "        for i, word1 in enumerate(unique_words):\n",
    "            for j, word2 in enumerate(unique_words):\n",
    "                if i < j:\n",
    "                    cooccur_freq[(word1, word2)] += 1\n",
    "                    cooccur_freq[(word2, word1)] += 1  # since it's symmetric\n",
    "    # return the two dictionaries\n",
    "    return word_freq, cooccur_freq\n",
    "\n",
    "# calculate the word frequencies and word co-occurences on our corpus\n",
    "word_freq, cooccur_freq = calculate_frequencies(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd816e-65a0-4e50-94f6-2fe808f506b6",
   "metadata": {},
   "source": [
    "#### 2.3. Calculate PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631ce77d-c2cf-4a21-a0f9-d1150ac42c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI(republicans, democrats) = 3.0387057970973492\n"
     ]
    }
   ],
   "source": [
    "# function that calculate the PMI between two words based on the calculated frequencies/co-occurences\n",
    "# INPUT: The two words, the frequency dictionary, the co-occurrence dictionary, and the number of documents in the corpus\n",
    "# OUTPUT: The PMI value of the two words\n",
    "def pmi(word1, word2, word_freq, cooccur_freq, total_docs):\n",
    "    \n",
    "    # calculate probability for word1\n",
    "    prob_word1 = word_freq[word1] / total_docs\n",
    "    \n",
    "    # calculate probability for word2\n",
    "    prob_word2 = word_freq[word2] / total_docs\n",
    "    \n",
    "    # calculate probability of co-occurence for word1 and word2\n",
    "    prob_word1_word2 = cooccur_freq[(word1, word2)] / total_docs\n",
    "\n",
    "    # calculate and return PMI \n",
    "    return math.log(prob_word1_word2 / (prob_word1 * prob_word2), 2) if prob_word1_word2 > 0 else 0\n",
    "\n",
    "# lets calculate the PMI on an example\n",
    "word1, word2 = \"republicans\", \"democrats\" \n",
    "print(f\"PMI({word1}, {word2}) =\", pmi(word1, word2, word_freq, cooccur_freq, len(preprocessed_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103ab614-6798-4013-9ab5-0cb6311ef61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI(election, trump) = 0.04765173853628407\n"
     ]
    }
   ],
   "source": [
    "word1, word2 = \"election\", \"trump\" \n",
    "print(f\"PMI({word1}, {word2}) =\", pmi(word1, word2, word_freq, cooccur_freq, len(preprocessed_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bb455-d9ba-4f6d-8a78-a7750115b9d1",
   "metadata": {},
   "source": [
    "This positive value indicates that the word \"election\" co-occurs with the word \"trump\" more often than would be expected if the two words were independent of each other. However, the value is relatively close to 0, suggesting that the association is not particularly strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a090712-ac1b-464e-8179-60ad4ed19343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI(election, clinton) = 1.330518018899047\n"
     ]
    }
   ],
   "source": [
    "word1, word2 = \"election\", \"clinton\" \n",
    "print(f\"PMI({word1}, {word2}) =\", pmi(word1, word2, word_freq, cooccur_freq, len(preprocessed_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7925b-17fe-4206-9dd4-f059afe26082",
   "metadata": {},
   "source": [
    "This value is considerably higher than the PMI for \"election\" and \"trump\", indicating a stronger association between the words \"election\" and \"clinton\". The positive value indicates that these two words co-occur more frequently together than would be expected under the assumption of independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63197fb8-c282-437e-9cd8-6c1a89ea35dd",
   "metadata": {},
   "source": [
    "Based on the PMI values, in the dataset of Trump's tweets, the word \"election\" is more strongly associated with the word \"clinton\" than with the word \"trump\". This suggests that when Trump mentioned the word \"election\", he was more likely to also mention \"clinton\" than to mention \"trump\" in the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c74de-fd40-48fd-adc0-074ac9a69cd8",
   "metadata": {},
   "source": [
    "### 3. Word2vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8475641-688d-4b7a-93c5-29c65460552f",
   "metadata": {},
   "source": [
    "Word2Vec is a popular NLP approach used for generating dense vector representations of words in large text corpora. These vectors aim to capture the semantic meaning of words, where words with similar meanings are located close to each other in the vector space.\n",
    "\n",
    "Word2Vec encompasses two main architectures:\n",
    "\n",
    "1. Continuous Bag of Words (CBOW): Predicts a word given its context.\n",
    "2. Skip-Gram: Predicts the context (surrounding words) given a word.\n",
    "\n",
    "### Key Features:\n",
    "1. **Fixed-size vectors:** Regardless of the vocabulary size, each word is represented as a fixed-size vector (e.g., 300 dimensions).\n",
    "2. **Cosine similarity:** Word vectors can be compared using cosine similarity to measure how similar they are in terms of meaning.\n",
    "3. **Semantic Relationships**: One of Word2Vec's famous properties is its ability to perform operations like \"King\" - \"Man\" + \"Woman\" ≈ \"Queen\".\n",
    "\n",
    "In this part of the lab, we are going to see how we can use Word2vec models and how we can train our own word2vec models using the Gensim Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41a643",
   "metadata": {},
   "source": [
    "##### Library Imports\n",
    "\n",
    "For Word2Vec model we are going to use the Gensim library. Gensim offers manmy pre-trained word2vec models and an easy interface for training and using Word2vec models. Lets check what pre-trained word2vec models are available on the Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ccc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim # Python library for using word embedding models\n",
    "import gensim.downloader # to download pretrained models\n",
    "from sklearn.manifold import TSNE # dimensionality reduction method\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import warnings\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "warnings.filterwarnings('ignore') # no warnings pls :)\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys())) #list the pretrained models available from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979089f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load a pretrained Word2Vec model\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4730f0",
   "metadata": {},
   "source": [
    "Having loaded a Word2vec model we are gonna go through some of the basic functionalities:\n",
    "1. Given a word, extract its vector\n",
    "2. Given a word, extract its most similar words\n",
    "3. Demonstrate relationships between word embeddings\n",
    "4. Visualize embeddings in a 2D space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd301e0d",
   "metadata": {},
   "source": [
    "#### 3.1. Extracting word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208f4cb-0ed7-439c-94ee-520e780dc1f3",
   "metadata": {},
   "source": [
    "After loading a model, we can convert a word into a dense vector based on the trained Word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2518f0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10205078, -0.08398438, -0.10546875,  0.09863281, -0.08935547,\n",
       "       -0.00842285, -0.09570312, -0.31835938,  0.12597656, -0.2578125 ,\n",
       "       -0.03442383, -0.31445312,  0.03808594,  0.17089844,  0.17773438,\n",
       "        0.12890625, -0.07714844,  0.01000977, -0.14648438,  0.06298828,\n",
       "        0.17480469,  0.05908203,  0.46875   , -0.09619141, -0.36328125,\n",
       "        0.08642578, -0.29101562,  0.14453125, -0.16992188, -0.07324219,\n",
       "       -0.3125    , -0.02258301,  0.00408936, -0.22558594, -0.08642578,\n",
       "       -0.16796875, -0.02600098,  0.09472656,  0.18359375,  0.08740234,\n",
       "        0.18847656, -0.16113281,  0.17285156,  0.29492188,  0.47265625,\n",
       "       -0.16796875,  0.01635742, -0.0859375 , -0.17871094, -0.01416016,\n",
       "       -0.16308594,  0.08642578, -0.11621094,  0.4375    ,  0.34960938,\n",
       "       -0.07958984, -0.01904297,  0.08447266, -0.03857422, -0.6171875 ,\n",
       "        0.15917969, -0.18652344, -0.02587891,  0.00159454, -0.15429688,\n",
       "       -0.00212097, -0.02441406,  0.06738281,  0.26367188,  0.2421875 ,\n",
       "       -0.171875  ,  0.37695312,  0.02807617,  0.04858398, -0.11474609,\n",
       "       -0.07421875, -0.13574219, -0.06787109,  0.21289062,  0.01794434,\n",
       "       -0.1484375 ,  0.34179688,  0.20605469, -0.30664062,  0.46679688,\n",
       "       -0.13964844, -0.02050781,  0.22070312,  0.01409912,  0.14355469,\n",
       "       -0.13574219,  0.09423828,  0.00717163, -0.31445312, -0.27929688,\n",
       "        0.31835938, -0.01086426, -0.24121094,  0.32617188,  0.16015625,\n",
       "       -0.08398438, -0.10791016, -0.04003906, -0.22265625,  0.09716797,\n",
       "       -0.04589844, -0.23144531, -0.09472656,  0.41796875,  0.04418945,\n",
       "       -0.45507812, -0.13183594, -0.04052734, -0.09423828,  0.24414062,\n",
       "       -0.08447266,  0.02648926, -0.06738281,  0.31640625,  0.16601562,\n",
       "        0.1328125 , -0.265625  , -0.47460938, -0.11572266,  0.29296875,\n",
       "       -0.11621094,  0.03881836,  0.12207031,  0.09863281, -0.14160156,\n",
       "       -0.43359375,  0.01409912, -0.29492188,  0.09228516, -0.15136719,\n",
       "       -0.02819824,  0.04858398,  0.11523438, -0.18554688, -0.08642578,\n",
       "        0.42773438,  0.03295898,  0.05883789, -0.49804688,  0.08105469,\n",
       "        0.08789062, -0.07763672, -0.07910156,  0.02807617, -0.31054688,\n",
       "        0.04003906,  0.22949219, -0.10644531, -0.14746094, -0.10791016,\n",
       "       -0.11181641,  0.08886719, -0.33203125,  0.11962891, -0.15039062,\n",
       "       -0.34765625,  0.30859375, -0.17675781,  0.30273438, -0.2734375 ,\n",
       "       -0.15234375,  0.06030273,  0.07763672, -0.25390625,  0.31640625,\n",
       "        0.10107422,  0.31640625,  0.09423828,  0.08105469, -0.06640625,\n",
       "        0.08886719,  0.02746582, -0.16699219,  0.14648438, -0.12207031,\n",
       "       -0.08056641,  0.09130859,  0.13964844,  0.05957031,  0.22558594,\n",
       "        0.02307129, -0.35742188,  0.19433594,  0.05175781, -0.19238281,\n",
       "        0.18164062,  0.28515625, -0.19628906, -0.13378906,  0.24023438,\n",
       "        0.31445312, -0.24023438,  0.11865234, -0.06152344, -0.15820312,\n",
       "        0.12304688,  0.26953125, -0.02722168,  0.24707031,  0.15136719,\n",
       "       -0.03112793, -0.05297852,  0.13867188, -0.11914062,  0.15136719,\n",
       "        0.08544922,  0.36132812, -0.0027771 , -0.36914062, -0.2109375 ,\n",
       "       -0.04492188,  0.10595703, -0.03491211, -0.28125   , -0.24414062,\n",
       "       -0.03112793,  0.12890625, -0.16992188,  0.05932617, -0.05151367,\n",
       "       -0.3828125 , -0.13378906,  0.13867188, -0.07177734,  0.10546875,\n",
       "        0.08251953,  0.3203125 , -0.06835938, -0.21484375,  0.40429688,\n",
       "       -0.359375  ,  0.10644531, -0.21191406,  0.10888672,  0.16992188,\n",
       "        0.14355469,  0.07617188, -0.03735352, -0.23535156, -0.0612793 ,\n",
       "       -0.03417969,  0.3203125 ,  0.24804688, -0.18457031, -0.3125    ,\n",
       "        0.01989746,  0.21386719,  0.02941895,  0.34765625,  0.4375    ,\n",
       "       -0.17285156, -0.01470947,  0.35742188, -0.296875  , -0.29492188,\n",
       "        0.08837891, -0.00860596,  0.18457031,  0.21289062, -0.125     ,\n",
       "       -0.13183594, -0.25195312, -0.07080078,  0.08105469, -0.06152344,\n",
       "        0.03051758,  0.26953125, -0.02355957, -0.390625  ,  0.1015625 ,\n",
       "       -0.04223633,  0.05737305, -0.07080078,  0.35351562,  0.45117188,\n",
       "        0.12792969,  0.04150391, -0.20410156,  0.00982666,  0.17382812,\n",
       "        0.29492188, -0.34960938,  0.5625    ,  0.38476562,  0.12792969,\n",
       "        0.4453125 ,  0.32226562,  0.10693359, -0.12304688, -0.07421875,\n",
       "       -0.5859375 ,  0.41992188, -0.15820312, -0.27734375, -0.3828125 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a vector for a specific word\n",
    "w2v.get_vector('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462ad6d1-44cb-470f-b97a-8ac8aaf31710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10205078, -0.08398438, -0.10546875,  0.09863281, -0.08935547,\n",
       "       -0.00842285, -0.09570312, -0.31835938,  0.12597656, -0.2578125 ,\n",
       "       -0.03442383, -0.31445312,  0.03808594,  0.17089844,  0.17773438,\n",
       "        0.12890625, -0.07714844,  0.01000977, -0.14648438,  0.06298828,\n",
       "        0.17480469,  0.05908203,  0.46875   , -0.09619141, -0.36328125,\n",
       "        0.08642578, -0.29101562,  0.14453125, -0.16992188, -0.07324219,\n",
       "       -0.3125    , -0.02258301,  0.00408936, -0.22558594, -0.08642578,\n",
       "       -0.16796875, -0.02600098,  0.09472656,  0.18359375,  0.08740234,\n",
       "        0.18847656, -0.16113281,  0.17285156,  0.29492188,  0.47265625,\n",
       "       -0.16796875,  0.01635742, -0.0859375 , -0.17871094, -0.01416016,\n",
       "       -0.16308594,  0.08642578, -0.11621094,  0.4375    ,  0.34960938,\n",
       "       -0.07958984, -0.01904297,  0.08447266, -0.03857422, -0.6171875 ,\n",
       "        0.15917969, -0.18652344, -0.02587891,  0.00159454, -0.15429688,\n",
       "       -0.00212097, -0.02441406,  0.06738281,  0.26367188,  0.2421875 ,\n",
       "       -0.171875  ,  0.37695312,  0.02807617,  0.04858398, -0.11474609,\n",
       "       -0.07421875, -0.13574219, -0.06787109,  0.21289062,  0.01794434,\n",
       "       -0.1484375 ,  0.34179688,  0.20605469, -0.30664062,  0.46679688,\n",
       "       -0.13964844, -0.02050781,  0.22070312,  0.01409912,  0.14355469,\n",
       "       -0.13574219,  0.09423828,  0.00717163, -0.31445312, -0.27929688,\n",
       "        0.31835938, -0.01086426, -0.24121094,  0.32617188,  0.16015625,\n",
       "       -0.08398438, -0.10791016, -0.04003906, -0.22265625,  0.09716797,\n",
       "       -0.04589844, -0.23144531, -0.09472656,  0.41796875,  0.04418945,\n",
       "       -0.45507812, -0.13183594, -0.04052734, -0.09423828,  0.24414062,\n",
       "       -0.08447266,  0.02648926, -0.06738281,  0.31640625,  0.16601562,\n",
       "        0.1328125 , -0.265625  , -0.47460938, -0.11572266,  0.29296875,\n",
       "       -0.11621094,  0.03881836,  0.12207031,  0.09863281, -0.14160156,\n",
       "       -0.43359375,  0.01409912, -0.29492188,  0.09228516, -0.15136719,\n",
       "       -0.02819824,  0.04858398,  0.11523438, -0.18554688, -0.08642578,\n",
       "        0.42773438,  0.03295898,  0.05883789, -0.49804688,  0.08105469,\n",
       "        0.08789062, -0.07763672, -0.07910156,  0.02807617, -0.31054688,\n",
       "        0.04003906,  0.22949219, -0.10644531, -0.14746094, -0.10791016,\n",
       "       -0.11181641,  0.08886719, -0.33203125,  0.11962891, -0.15039062,\n",
       "       -0.34765625,  0.30859375, -0.17675781,  0.30273438, -0.2734375 ,\n",
       "       -0.15234375,  0.06030273,  0.07763672, -0.25390625,  0.31640625,\n",
       "        0.10107422,  0.31640625,  0.09423828,  0.08105469, -0.06640625,\n",
       "        0.08886719,  0.02746582, -0.16699219,  0.14648438, -0.12207031,\n",
       "       -0.08056641,  0.09130859,  0.13964844,  0.05957031,  0.22558594,\n",
       "        0.02307129, -0.35742188,  0.19433594,  0.05175781, -0.19238281,\n",
       "        0.18164062,  0.28515625, -0.19628906, -0.13378906,  0.24023438,\n",
       "        0.31445312, -0.24023438,  0.11865234, -0.06152344, -0.15820312,\n",
       "        0.12304688,  0.26953125, -0.02722168,  0.24707031,  0.15136719,\n",
       "       -0.03112793, -0.05297852,  0.13867188, -0.11914062,  0.15136719,\n",
       "        0.08544922,  0.36132812, -0.0027771 , -0.36914062, -0.2109375 ,\n",
       "       -0.04492188,  0.10595703, -0.03491211, -0.28125   , -0.24414062,\n",
       "       -0.03112793,  0.12890625, -0.16992188,  0.05932617, -0.05151367,\n",
       "       -0.3828125 , -0.13378906,  0.13867188, -0.07177734,  0.10546875,\n",
       "        0.08251953,  0.3203125 , -0.06835938, -0.21484375,  0.40429688,\n",
       "       -0.359375  ,  0.10644531, -0.21191406,  0.10888672,  0.16992188,\n",
       "        0.14355469,  0.07617188, -0.03735352, -0.23535156, -0.0612793 ,\n",
       "       -0.03417969,  0.3203125 ,  0.24804688, -0.18457031, -0.3125    ,\n",
       "        0.01989746,  0.21386719,  0.02941895,  0.34765625,  0.4375    ,\n",
       "       -0.17285156, -0.01470947,  0.35742188, -0.296875  , -0.29492188,\n",
       "        0.08837891, -0.00860596,  0.18457031,  0.21289062, -0.125     ,\n",
       "       -0.13183594, -0.25195312, -0.07080078,  0.08105469, -0.06152344,\n",
       "        0.03051758,  0.26953125, -0.02355957, -0.390625  ,  0.1015625 ,\n",
       "       -0.04223633,  0.05737305, -0.07080078,  0.35351562,  0.45117188,\n",
       "        0.12792969,  0.04150391, -0.20410156,  0.00982666,  0.17382812,\n",
       "        0.29492188, -0.34960938,  0.5625    ,  0.38476562,  0.12792969,\n",
       "        0.4453125 ,  0.32226562,  0.10693359, -0.12304688, -0.07421875,\n",
       "       -0.5859375 ,  0.41992188, -0.15820312, -0.27734375, -0.3828125 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or simply\n",
    "w2v['twitter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827f358",
   "metadata": {},
   "source": [
    "#### 3.2. Extracting word similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da646d-640c-482a-9f14-8dbdc369ccbe",
   "metadata": {},
   "source": [
    "After obtaining dense vector representations for words, we can assess the similarity between words. For instance, we can find the most similar words to a word of interest. Gensim offers an easy-to-use interface to do this. Lets see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bec4f277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twitter', 0.8908904194831848),\n",
       " ('Twitter.com', 0.7536780834197998),\n",
       " ('tweet', 0.7431627511978149),\n",
       " ('tweeting', 0.7161932587623596),\n",
       " ('tweeted', 0.7137226462364197),\n",
       " ('facebook', 0.6988551616668701),\n",
       " ('tweets', 0.6974530220031738),\n",
       " ('Tweeted', 0.6950210332870483),\n",
       " ('Tweet', 0.6875007152557373),\n",
       " ('Tweeting', 0.6845167279243469)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the most similar words to a term\n",
    "w2v.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f07db27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twitter', 0.8908904194831848),\n",
       " ('Twitter.com', 0.7536780834197998),\n",
       " ('tweet', 0.7431627511978149),\n",
       " ('tweeting', 0.7161932587623596),\n",
       " ('tweeted', 0.7137226462364197),\n",
       " ('facebook', 0.6988551616668701),\n",
       " ('tweets', 0.6974530220031738),\n",
       " ('Tweeted', 0.6950210332870483),\n",
       " ('Tweet', 0.6875007152557373),\n",
       " ('Tweeting', 0.6845167279243469),\n",
       " ('Tweets', 0.6546129584312439),\n",
       " ('Facebook', 0.6483454704284668),\n",
       " ('TwitterTwitter', 0.6432195901870728),\n",
       " ('twittered', 0.6393427848815918),\n",
       " ('micro_blogging_site', 0.6302406787872314),\n",
       " ('Twitters', 0.62877357006073),\n",
       " ('twitterverse', 0.6279213428497314),\n",
       " ('blog', 0.6257337927818298),\n",
       " ('Follow_AndroidGuys', 0.6226419806480408),\n",
       " ('onTwitter', 0.6222246289253235)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the most similar words to a term\n",
    "w2v.most_similar('twitter', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d2ed8-3131-45c7-a08a-69483a79b287",
   "metadata": {},
   "source": [
    "We can also assess the similarity between two words of interest using the lines below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a156c01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34480923"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similarity('twitter', '4chan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f29f383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744767"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similarity('twitter', 'myspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698a878-9141-4b45-ad6e-02a7ba1ab25a",
   "metadata": {},
   "source": [
    "Based on this Word2vec model, MySpace is more semantically similar to Twitter than 4chan!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12c4d4",
   "metadata": {},
   "source": [
    "#### 3.3. Word Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd083b4",
   "metadata": {},
   "source": [
    "\n",
    "**most_similar_cosmul**: Find the top-N most similar words, using the multiplicative combination objective, proposed by Omer Levy and Yoav Goldberg “Linguistic Regularities in Sparse and Explicit Word Representations”. Positive words still contribute positively towards the similarity, negative words negatively, but with less susceptibility to one large distance dominating the calculation. In the common analogy-solving case, of two positive and one negative examples, this method is equivalent to the “3CosMul” objective (equation (4)) of Levy and Goldberg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733c73a-516d-4c1b-a87a-92a8ac0059f8",
   "metadata": {},
   "source": [
    "Lets demonstrate the popular example of King and Queen! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134ed3e-d315-423e-bfe0-a9df44b83dd2",
   "metadata": {},
   "source": [
    "w2v.most_similar_cosmul(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd3b11-e220-4365-a241-a91eca0d9f03",
   "metadata": {},
   "source": [
    "Do relationships work with countries and cities as well? Lets try with Greece and Italy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbaacd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('athens', 0.8228010535240173),\n",
       " ('samuel', 0.8189568519592285),\n",
       " ('cro', 0.8184024691581726),\n",
       " ('caroline', 0.8162517547607422),\n",
       " ('sess', 0.816144585609436),\n",
       " ('zz', 0.8157657980918884),\n",
       " ('afta', 0.8151275515556335),\n",
       " ('tting', 0.8136284947395325),\n",
       " ('hev', 0.8132066130638123),\n",
       " ('YEO_MAN', 0.8131966590881348)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar_cosmul(positive=['greece', 'rome'], negative=['italy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0303f5",
   "metadata": {},
   "source": [
    "#### 3.4. Embedding visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cdaff-f3c1-4eb7-833b-3be77f64649c",
   "metadata": {},
   "source": [
    "Lets visualize the embeddings in a 2D dimensional space to observe how they are represented in the dimensional space. We are going to use TSNE, which is a dimensionality reduction method that will help us convert our dense representation to 2-dimensions. Then, we are simply going to plot the two dimensions using matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6807af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# method to visualize words in a 2D Space\n",
    "\n",
    "def visualize_embeddings_using_tsne(model, target_words):\n",
    "    # extract the embeddings given the target words\n",
    "    embeddings = [model[x] for x in target_words]\n",
    "\n",
    "    \n",
    "    # reduce the dimensionality of the embeddings. From 300 to 2\n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=128)\n",
    "    coordinates = tsne_model.fit_transform(np.array(embeddings))\n",
    "\n",
    "    # extract the x,y coordinates from the reduced embeddings\n",
    "    x = [i[0] for i in coordinates]\n",
    "    y = [i[1] for i in coordinates]\n",
    "    \n",
    "    # plot the reducted embeddings\n",
    "    plt.figure(figsize=(8,8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(target_words[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "targets = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\",\n",
    "            \"rome\", \"italy\", \"paris\", \"france\" ]\n",
    "\n",
    "visualize_embeddings_using_tsne(w2v, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8479b9",
   "metadata": {},
   "source": [
    "#### 3.5 Training our own word2vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7fc7b-e084-4a53-a098-4986d0a766b9",
   "metadata": {},
   "source": [
    "We will use the same pre-processed corpus that includes Trump's tweets to train a Word2Vec model based on Trump's tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e21256a-921e-4d0a-87ba-1d639f5a8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republicans and Democrats have both created our economic problems. ['republicans', 'democrats', 'created', 'economic', 'problems']\n"
     ]
    }
   ],
   "source": [
    "print(docs[0], preprocessed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b79482d-9dee-46d5-8150-d0d984343e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was thrilled to be back in the Great city of Charlotte, North Carolina with thousands of hardworking American Patriots who love our Country, cherish our values, respect our laws, and always put AMERICA FIRST! Thank you for a wonderful evening!! #KAG2020 https://t.co/dNJZfRsl9y ['thrilled', 'back', 'great', 'city', 'charlotte', 'north', 'carolina', 'thousands', 'hardworking', 'american', 'patriots', 'love', 'country', 'cherish', 'values', 'respect', 'laws', 'always', 'put', 'america', 'first', 'thank', 'wonderful', 'evening', 'kag']\n"
     ]
    }
   ],
   "source": [
    "print(docs[1], preprocessed_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38d99b0b-c26d-4cad-b930-b0f9366ebc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the parameters of our Word2Vec model\n",
    "import gensim\n",
    "our_w2v_model = gensim.models.Word2Vec(sentences=preprocessed_docs,\n",
    "                                        vector_size=100, #dimensionality of the vectors\n",
    "                                        min_count=10, #Ignore all words with frequency less than this\n",
    "                                        window=5, #Size of the context window \n",
    "                                        epochs= 5, #Number of iterations over the corpus\n",
    "                                        sg=1, #Whether to use Skip-gram or CBOW (1 for skip-gram, 0 for CBOW)\n",
    "                                        workers=5, #Number of workers to train the model (faster training on multicore machines)\n",
    "                                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a038ae4-9713-4209-9a28-8936e739198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clinton', 0.8819814324378967),\n",
       " ('crooked', 0.8376810550689697),\n",
       " ('dnc', 0.7517157196998596),\n",
       " ('hillarys', 0.7216495871543884),\n",
       " ('acid', 0.7194294929504395),\n",
       " ('h', 0.7148081064224243),\n",
       " ('firm', 0.7102262377738953),\n",
       " ('clintons', 0.7070257663726807),\n",
       " ('emails', 0.7049728631973267),\n",
       " ('dirt', 0.7003675699234009)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_w2v_model.wv.most_similar('hillary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be8a1de1-fcfe-4149-a368-9bf6ec6425ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having trained a model we can save it to use it later\n",
    "our_w2v_model.save('trump-tweets.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82c9efb2-62fb-4696-b99a-e326ca5c8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load a model with \n",
    "trump_model = gensim.models.Word2Vec.load('trump-tweets.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62ad5300-661a-4cf1-b365-4cee8044f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clinton', 0.8819814324378967),\n",
       " ('crooked', 0.8376810550689697),\n",
       " ('dnc', 0.7517157196998596),\n",
       " ('hillarys', 0.7216495871543884),\n",
       " ('acid', 0.7194294929504395),\n",
       " ('h', 0.7148081064224243),\n",
       " ('firm', 0.7102262377738953),\n",
       " ('clintons', 0.7070257663726807),\n",
       " ('emails', 0.7049728631973267),\n",
       " ('dirt', 0.7003675699234009)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_model.wv.most_similar('hillary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f55dceec-4462-406b-80ef-ad537382bec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6220"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can extract the words in the vocabulary using\n",
    "vocabulary_words = [key for key,value in trump_model.wv.key_to_index.items()]\n",
    "len(vocabulary_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1aefd-92ec-4cca-aba0-f1f3575dda08",
   "metadata": {},
   "source": [
    "Our Word2vec model has 6,220 words in its vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34c1680d-8813-4eae-b9f8-c24ada057303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in vocabulary: realdonaldtrump\n"
     ]
    }
   ],
   "source": [
    "print(\"First word in vocabulary: %s\" %(vocabulary_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "897faab8-1538-483f-8098-84ca1e4c04b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10889"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding out how many times a word appeared in the corpus\n",
    "trump_model.wv.get_vecattr('realdonaldtrump', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e0ebe-4b37-4410-80a6-5635c2aa9c8f",
   "metadata": {},
   "source": [
    "### **Exercise: Using the trained Word2Vec model \"word2vec-google-news-300\", use the Gensim Word2Vec interface to answer the following questions:**\n",
    "1. Use the Word2Vec model to find the top 20 words most similar to \"computers\". How do these words reflect different aspects or contexts of \"computers\"?\n",
    "2.  Measure the semantic similarity between \"peace\" and \"war\" compared to \"sun\" and \"moon\" using the Word2Vec model. Think what the differences in similarity tell you about the relationship and contextual connections between these pairs of words in the Google News dataset.\n",
    "3. Use the Word2Vec model to solve the analogy \"book is to reading as music is to what?\" Analyze the top results to understand how the model associates music with certain activities or concepts, and think the rationale behind these associations based on the context provided in the Google News dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc943642-0761-4a5d-8cc1-a36cd7d582d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a76133-850b-4d4a-8dbf-3c71df354cf0",
   "metadata": {},
   "source": [
    "## TI3160TU: Natural Language Processing - Vector Semantics Lab -- END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
